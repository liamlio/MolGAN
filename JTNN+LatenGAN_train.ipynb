{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "JTNN+LatenGAN_train",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "k85YYlp3wi9a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "9f2f79b7-4d98-4008-cdbe-11479e946cbf"
      },
      "source": [
        "!wget http://molcyclegan.ardigen.com/250k_rndm_zinc_drugs_clean_3_canonized.csv\n",
        "!wget http://molcyclegan.ardigen.com/X_JTVAE_250k_rndm_zinc.csv"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-03-15 18:16:16--  http://molcyclegan.ardigen.com/250k_rndm_zinc_drugs_clean_3_canonized.csv\n",
            "Resolving molcyclegan.ardigen.com (molcyclegan.ardigen.com)... 188.128.194.238\n",
            "Connecting to molcyclegan.ardigen.com (molcyclegan.ardigen.com)|188.128.194.238|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 22101155 (21M) [text/csv]\n",
            "Saving to: ‘250k_rndm_zinc_drugs_clean_3_canonized.csv.1’\n",
            "\n",
            "250k_rndm_zinc_drug 100%[===================>]  21.08M  8.29MB/s    in 2.5s    \n",
            "\n",
            "2020-03-15 18:16:19 (8.29 MB/s) - ‘250k_rndm_zinc_drugs_clean_3_canonized.csv.1’ saved [22101155/22101155]\n",
            "\n",
            "--2020-03-15 18:16:19--  http://molcyclegan.ardigen.com/X_JTVAE_250k_rndm_zinc.csv\n",
            "Resolving molcyclegan.ardigen.com (molcyclegan.ardigen.com)... 188.128.194.238\n",
            "Connecting to molcyclegan.ardigen.com (molcyclegan.ardigen.com)|188.128.194.238|:80... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 164441376 (157M) [text/csv]\n",
            "Saving to: ‘X_JTVAE_250k_rndm_zinc.csv.1’\n",
            "\n",
            "X_JTVAE_250k_rndm_z 100%[===================>] 156.82M  11.4MB/s    in 16s     \n",
            "\n",
            "2020-03-15 18:16:35 (9.95 MB/s) - ‘X_JTVAE_250k_rndm_zinc.csv.1’ saved [164441376/164441376]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TjywBaDEVFCx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import torch.optim as optim"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cTrQarwwOk5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "data = pd.read_csv('X_JTVAE_250k_rndm_zinc.csv')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jxlFYRMHTPf0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "smiles = data['SMILES'].values\n",
        "np.savetxt(r'smiles.txt', smiles, fmt='%s')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mh-408HFrktC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self, data_shape):\n",
        "        super(Discriminator, self).__init__()\n",
        "        self.data_shape = data_shape\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(int(np.prod(self.data_shape)), 512),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(512, 256),\n",
        "            nn.LeakyReLU(0.2, inplace=True),\n",
        "            nn.Linear(256, 1),\n",
        "        )\n",
        "\n",
        "    def forward(self, mol):\n",
        "        validity = self.model(mol)\n",
        "        return validity\n",
        "\n",
        "    def save(self, path):\n",
        "        save_dict = {\n",
        "            'model': self.model.state_dict(),\n",
        "            'data_shape': self.data_shape,\n",
        "        }\n",
        "        torch.save(save_dict, path)\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "        save_dict = torch.load(path)\n",
        "        D = Discriminator(save_dict['data_shape'])\n",
        "        D.model.load_state_dict(save_dict[\"model\"])\n",
        "\n",
        "        return D"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0ZmwIN5VPeZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "\n",
        "class Generator(nn.Module):\n",
        "    def __init__(self, data_shape, latent_dim=None):\n",
        "        super(Generator, self).__init__()\n",
        "        self.data_shape = data_shape\n",
        "\n",
        "        # latent dim of the generator is one of the hyperparams.\n",
        "        # by default it is set to the prod of data_shapes\n",
        "        self.latent_dim = int(np.prod(self.data_shape)) if latent_dim is None else latent_dim\n",
        "\n",
        "        def block(in_feat, out_feat, normalize=True):\n",
        "            layers = [nn.Linear(in_feat, out_feat)]\n",
        "            if normalize:\n",
        "                layers.append(nn.BatchNorm1d(out_feat, 0.8))\n",
        "            layers.append(nn.LeakyReLU(0.2, inplace=True))\n",
        "            return layers\n",
        "\n",
        "        self.model = nn.Sequential(\n",
        "            *block(self.latent_dim, 128, normalize=False),\n",
        "            *block(128, 256),\n",
        "            *block(256, 512),\n",
        "            *block(512, 1024),\n",
        "            nn.Linear(1024, int(np.prod(self.data_shape))),\n",
        "            # nn.Tanh() # expecting latent vectors to be not normalized\n",
        "        )\n",
        "\n",
        "    def forward(self, z):\n",
        "        out = self.model(z)\n",
        "        return out\n",
        "\n",
        "    def save(self, path):\n",
        "        save_dict = {\n",
        "            'latent_dim': self.latent_dim,\n",
        "            'model': self.model.state_dict(),\n",
        "            'data_shape': self.data_shape,\n",
        "        }\n",
        "        torch.save(save_dict, path)\n",
        "\n",
        "        return\n",
        "\n",
        "    @staticmethod\n",
        "    def load(path):\n",
        "        save_dict = torch.load(path)\n",
        "        G = Generator(save_dict['data_shape'], latent_dim=save_dict['latent_dim'])\n",
        "        G.model.load_state_dict(save_dict[\"model\"])\n",
        "\n",
        "        return G"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_NWvRUvVS5G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Sampler(object):\n",
        "    \"\"\"\n",
        "    Sampling the mols the generator.\n",
        "    All scripts should use this class for sampling.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, generator: Generator):\n",
        "        self.set_generator(generator)\n",
        "\n",
        "    def set_generator(self, generator):\n",
        "        self.G = generator\n",
        "\n",
        "    def sample(self, n):\n",
        "        # Sample noise as generator input\n",
        "        z = torch.cuda.FloatTensor(np.random.uniform(-1, 1, (n, self.G.latent_dim)))\n",
        "        # Generate a batch of mols\n",
        "        return self.G(z)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qlo1tKSDAuHo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch.utils import data\n",
        "import json\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "class LatentMolsDataset(data.Dataset):\n",
        "    def __init__(self, latent_space_mols):\n",
        "        self.data = latent_space_mols\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.data[index]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xjPf5FRV4xT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pickle\n",
        "import os\n",
        "import torch\n",
        "import torch.autograd as autograd\n",
        "import numpy as np\n",
        "import json\n",
        "import time\n",
        "import sys\n",
        "\n",
        "\n",
        "class TrainModelRunner:\n",
        "    # Loss weight for gradient penalty\n",
        "    lambda_gp = 10\n",
        "\n",
        "    def __init__(self, input_data_path, output_model_folder, decode_mols_save_path='', n_epochs=2000, starting_epoch=1,\n",
        "                 batch_size=2500, lr=0.0002, b1=0.5, b2=0.999,  n_critic=5,\n",
        "                 save_interval=1000, sample_after_training=30000, message=\"\"):\n",
        "        self.message = message\n",
        "\n",
        "        # init params\n",
        "        self.input_data_path = input_data_path\n",
        "        self.output_model_folder = output_model_folder\n",
        "        self.n_epochs = n_epochs\n",
        "        self.starting_epoch = starting_epoch\n",
        "        self.batch_size = batch_size\n",
        "        self.lr = lr\n",
        "        self.b1 = b1\n",
        "        self.b2 = b2\n",
        "        self.n_critic = n_critic\n",
        "        self.save_interval = save_interval\n",
        "        self.sample_after_training = sample_after_training\n",
        "        self.decode_mols_save_path = decode_mols_save_path\n",
        "\n",
        "        # initialize dataloader\n",
        "        smiles_lat = pd.read_csv(input_data_path)\n",
        "        latent_space_mols = smiles_lat.drop('SMILES', axis=1).values\n",
        "        latent_space_mols = latent_space_mols.reshape(latent_space_mols.shape[0], 56)\n",
        "\n",
        "        self.dataloader = torch.utils.data.DataLoader(LatentMolsDataset(latent_space_mols), shuffle=True,\n",
        "                                                      batch_size=self.batch_size, drop_last=True)\n",
        "\n",
        "        # load discriminator\n",
        "        discriminator_name = 'discriminator.txt' if self.starting_epoch == 1 else str(\n",
        "            self.starting_epoch) + '_discriminator.txt'\n",
        "        discriminator_path = os.path.join(output_model_folder, discriminator_name)\n",
        "        self.D = Discriminator.load(discriminator_path)\n",
        "        # self.D = Discriminator(latent_space_mols[0].shape)\n",
        "        # load generator\n",
        "        generator_name = 'generator.txt' if self.starting_epoch == 1 else str(\n",
        "            self.starting_epoch) + '_generator.txt'\n",
        "        generator_path = os.path.join(output_model_folder, generator_name)\n",
        "        self.G = Generator(.load(generator_path)\n",
        "        # self.G = Generator(latent_space_mols[0].shape)\n",
        "        # initialize sampler\n",
        "        self.Sampler = Sampler(self.G)\n",
        "\n",
        "        # initialize optimizer\n",
        "        self.optimizer_G = torch.optim.Adam(self.G.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
        "        self.optimizer_D = torch.optim.Adam(self.D.parameters(), lr=self.lr, betas=(self.b1, self.b2))\n",
        "\n",
        "        # Tensor\n",
        "        cuda = True if torch.cuda.is_available() else False\n",
        "        if cuda:\n",
        "            self.G.cuda()\n",
        "            self.D.cuda()\n",
        "        self.Tensor = torch.cuda.FloatTensor if cuda else torch.FloatTensor\n",
        "\n",
        "    def run(self):\n",
        "\n",
        "        print(\"Run began.\")\n",
        "        print(\"Message: %s\" % self.message)\n",
        "        sys.stdout.flush()\n",
        "\n",
        "        batches_done = 0\n",
        "        disc_loss_log = []\n",
        "        g_loss_log = []\n",
        "\n",
        "        for epoch in range(self.starting_epoch, self.n_epochs + self.starting_epoch):\n",
        "            disc_loss_per_batch = []\n",
        "            g_loss_log_per_batch = []\n",
        "            for i, real_mols in enumerate(self.dataloader):\n",
        "\n",
        "                # Configure input\n",
        "                real_mols = real_mols.type(self.Tensor)\n",
        "                # real_mols = np.squeeze(real_mols, axis=1)\n",
        "\n",
        "                # ---------------------\n",
        "                #  Train Discriminator\n",
        "                # ---------------------\n",
        "\n",
        "                self.optimizer_D.zero_grad()\n",
        "\n",
        "                # Generate a batch of mols from noise\n",
        "                fake_mols = self.Sampler.sample(real_mols.shape[0])\n",
        "\n",
        "                # Real mols\n",
        "                real_validity = self.D(real_mols)\n",
        "                # Fake mols\n",
        "                fake_validity = self.D(fake_mols)\n",
        "                # Gradient penalty\n",
        "                gradient_penalty = self.compute_gradient_penalty(real_mols.data, fake_mols.data)\n",
        "                # Adversarial loss\n",
        "                d_loss = -torch.mean(real_validity) + torch.mean(fake_validity) + self.lambda_gp * gradient_penalty\n",
        "                disc_loss_per_batch.append(d_loss.item())\n",
        "\n",
        "                d_loss.backward()\n",
        "                self.optimizer_D.step()\n",
        "                self.optimizer_G.zero_grad()\n",
        "\n",
        "                # Train the generator every n_critic steps\n",
        "                if i % self.n_critic == 0:\n",
        "                    # -----------------\n",
        "                    #  Train Generator\n",
        "                    # -----------------\n",
        "\n",
        "                    # Generate a batch of mols\n",
        "                    fake_mols = self.Sampler.sample(real_mols.shape[0])\n",
        "                    # Loss measures generator's ability to fool the discriminator\n",
        "                    # Train on fake images\n",
        "                    fake_validity = self.D(fake_mols)\n",
        "                    g_loss = -torch.mean(fake_validity)\n",
        "                    g_loss_log_per_batch.append(g_loss.item())\n",
        "\n",
        "                    g_loss.backward()\n",
        "                    self.optimizer_G.step()\n",
        "\n",
        "                    batches_done += self.n_critic\n",
        "\n",
        "                # If last batch in the set\n",
        "                if i == len(self.dataloader) - 1:\n",
        "                    if epoch % self.save_interval == 0:\n",
        "                        generator_save_path = os.path.join(self.output_model_folder,\n",
        "                                                           str(epoch) + '_generator.txt')\n",
        "                        discriminator_save_path = os.path.join(self.output_model_folder,\n",
        "                                                               str(epoch) + '_discriminator.txt')\n",
        "                        self.G.save(generator_save_path)\n",
        "                        self.D.save(discriminator_save_path)\n",
        "\n",
        "                    disc_loss_log.append([time.time(), epoch, np.mean(disc_loss_per_batch)])\n",
        "                    g_loss_log.append([time.time(), epoch, np.mean(g_loss_log_per_batch)])\n",
        "\n",
        "                    # Print and log\n",
        "                    print(\n",
        "                        \"[Epoch %d/%d]  [Disc loss: %f] [Gen loss: %f] \"\n",
        "                        % (epoch, self.n_epochs + self.starting_epoch, disc_loss_log[-1][2], g_loss_log[-1][2])\n",
        "                    )\n",
        "                    sys.stdout.flush()\n",
        "\n",
        "        # log the losses\n",
        "        with open(os.path.join(self.output_model_folder, 'disc_loss.json'), 'w') as json_file:\n",
        "            json.dump(disc_loss_log, json_file)\n",
        "        with open(os.path.join(self.output_model_folder, 'gen_loss.json'), 'w') as json_file:\n",
        "            json.dump(g_loss_log, json_file)\n",
        "\n",
        "        # Sampling after training\n",
        "        if self.sample_after_training > 0:\n",
        "            print(\"Training finished. Generating sample of latent vectors\")\n",
        "            # sampling mode\n",
        "            torch.no_grad()\n",
        "            self.G.eval()\n",
        "\n",
        "            S = Sampler(generator=self.G)\n",
        "            latent = S.sample(self.sample_after_training)\n",
        "            latent = latent.detach().cpu().numpy().tolist()\n",
        "\n",
        "            sampled_mols_save_path = os.path.join(self.output_model_folder, 'sampled')\n",
        "            np.save(sampled_mols_save_path+f'_epoch{epoch}', latent)\n",
        "\n",
        "            # decoding sampled mols\n",
        "\n",
        "        return 0\n",
        "\n",
        "    def compute_gradient_penalty(self, real_samples, fake_samples):\n",
        "        \"\"\"Calculates the gradient penalty loss for WGAN GP\"\"\"\n",
        "        # Random weight term for interpolation between real and fake samples\n",
        "        alpha = self.Tensor(np.random.random((real_samples.size(0), 1)))\n",
        "\n",
        "        # Get random interpolation between real and fake samples\n",
        "        interpolates = (alpha * real_samples + ((1 - alpha) * fake_samples)).requires_grad_(True)\n",
        "        d_interpolates = self.D(interpolates)\n",
        "        fake = self.Tensor(real_samples.shape[0], 1).fill_(1.0)\n",
        "\n",
        "        # Get gradient w.r.t. interpolates\n",
        "        gradients = autograd.grad(\n",
        "            outputs=d_interpolates,\n",
        "            inputs=interpolates,\n",
        "            grad_outputs=fake,\n",
        "            create_graph=True,\n",
        "            retain_graph=True,\n",
        "            only_inputs=True,\n",
        "        )[0]\n",
        "        gradients = gradients.view(gradients.size(0), -1)\n",
        "        gradient_penalty = ((gradients.norm(2, dim=1) - 1) ** 2).mean()\n",
        "\n",
        "        return gradient_penalty"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MGNNUWy1Az_B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainer = TrainModelRunner('/content/X_JTVAE_250k_rndm_zinc.csv', output_model_folder='/content/model', starting_epoch=200,\n",
        "                           save_interval=100, message='Starting training', batch_size=2500)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gAV2y8c_kHWc",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "a83a21ea-37b5-417c-c238-b00d1480b819"
      },
      "source": [
        "trainer.run()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Run began.\n",
            "Message: Starting training\n",
            "[Epoch 200/2200]  [Disc loss: -5.017041] [Gen loss: -0.352216] \n",
            "[Epoch 201/2200]  [Disc loss: -10.961712] [Gen loss: -1.055464] \n",
            "[Epoch 202/2200]  [Disc loss: -9.807225] [Gen loss: -0.696715] \n",
            "[Epoch 203/2200]  [Disc loss: -8.607686] [Gen loss: 0.060340] \n",
            "[Epoch 204/2200]  [Disc loss: -8.559876] [Gen loss: -0.505259] \n",
            "[Epoch 205/2200]  [Disc loss: -8.005614] [Gen loss: -0.392123] \n",
            "[Epoch 206/2200]  [Disc loss: -7.827546] [Gen loss: -0.499194] \n",
            "[Epoch 207/2200]  [Disc loss: -7.369315] [Gen loss: -0.555965] \n",
            "[Epoch 208/2200]  [Disc loss: -6.921161] [Gen loss: -0.694019] \n",
            "[Epoch 209/2200]  [Disc loss: -6.528371] [Gen loss: -0.980572] \n",
            "[Epoch 210/2200]  [Disc loss: -6.317981] [Gen loss: -0.929785] \n",
            "[Epoch 211/2200]  [Disc loss: -5.947452] [Gen loss: -0.827895] \n",
            "[Epoch 212/2200]  [Disc loss: -5.440610] [Gen loss: -0.762877] \n",
            "[Epoch 213/2200]  [Disc loss: -5.192881] [Gen loss: -0.232353] \n",
            "[Epoch 214/2200]  [Disc loss: -5.134250] [Gen loss: -0.235080] \n",
            "[Epoch 215/2200]  [Disc loss: -5.081403] [Gen loss: -0.714943] \n",
            "[Epoch 216/2200]  [Disc loss: -4.880120] [Gen loss: -0.476513] \n",
            "[Epoch 217/2200]  [Disc loss: -4.704821] [Gen loss: -0.450339] \n",
            "[Epoch 218/2200]  [Disc loss: -4.530038] [Gen loss: -0.473468] \n",
            "[Epoch 219/2200]  [Disc loss: -4.374415] [Gen loss: -0.521149] \n",
            "[Epoch 220/2200]  [Disc loss: -4.025223] [Gen loss: -0.561843] \n",
            "[Epoch 221/2200]  [Disc loss: -3.925795] [Gen loss: -0.151995] \n",
            "[Epoch 222/2200]  [Disc loss: -3.904253] [Gen loss: -0.024956] \n",
            "[Epoch 223/2200]  [Disc loss: -3.940362] [Gen loss: 0.034093] \n",
            "[Epoch 224/2200]  [Disc loss: -4.037410] [Gen loss: -0.045827] \n",
            "[Epoch 225/2200]  [Disc loss: -3.701595] [Gen loss: -0.021928] \n",
            "[Epoch 226/2200]  [Disc loss: -3.723526] [Gen loss: -0.100332] \n",
            "[Epoch 227/2200]  [Disc loss: -3.703090] [Gen loss: 0.015572] \n",
            "[Epoch 228/2200]  [Disc loss: -3.811382] [Gen loss: -0.111449] \n",
            "[Epoch 229/2200]  [Disc loss: -3.792235] [Gen loss: 0.081344] \n",
            "[Epoch 230/2200]  [Disc loss: -3.684087] [Gen loss: -0.089867] \n",
            "[Epoch 231/2200]  [Disc loss: -3.678284] [Gen loss: -0.065724] \n",
            "[Epoch 232/2200]  [Disc loss: -3.663446] [Gen loss: -0.233045] \n",
            "[Epoch 233/2200]  [Disc loss: -3.641567] [Gen loss: -0.345498] \n",
            "[Epoch 234/2200]  [Disc loss: -3.603720] [Gen loss: -0.037739] \n",
            "[Epoch 235/2200]  [Disc loss: -3.740910] [Gen loss: -0.223345] \n",
            "[Epoch 236/2200]  [Disc loss: -3.540832] [Gen loss: -0.422719] \n",
            "[Epoch 237/2200]  [Disc loss: -3.523251] [Gen loss: -0.203432] \n",
            "[Epoch 238/2200]  [Disc loss: -3.572305] [Gen loss: -0.055942] \n",
            "[Epoch 239/2200]  [Disc loss: -3.517419] [Gen loss: 0.015189] \n",
            "[Epoch 240/2200]  [Disc loss: -3.572458] [Gen loss: 0.011041] \n",
            "[Epoch 241/2200]  [Disc loss: -3.516521] [Gen loss: 0.306754] \n",
            "[Epoch 242/2200]  [Disc loss: -3.450153] [Gen loss: 0.065120] \n",
            "[Epoch 243/2200]  [Disc loss: -3.401613] [Gen loss: -0.025090] \n",
            "[Epoch 244/2200]  [Disc loss: -3.251059] [Gen loss: 0.478297] \n",
            "[Epoch 245/2200]  [Disc loss: -3.313743] [Gen loss: 0.181706] \n",
            "[Epoch 246/2200]  [Disc loss: -3.157013] [Gen loss: 0.003687] \n",
            "[Epoch 247/2200]  [Disc loss: -3.134795] [Gen loss: 0.292341] \n",
            "[Epoch 248/2200]  [Disc loss: -3.092691] [Gen loss: 0.059656] \n",
            "[Epoch 249/2200]  [Disc loss: -2.966076] [Gen loss: 0.368408] \n",
            "[Epoch 250/2200]  [Disc loss: -2.930228] [Gen loss: 0.185300] \n",
            "[Epoch 251/2200]  [Disc loss: -2.818892] [Gen loss: 0.569694] \n",
            "[Epoch 252/2200]  [Disc loss: -2.828107] [Gen loss: 0.590824] \n",
            "[Epoch 253/2200]  [Disc loss: -2.813336] [Gen loss: 0.737498] \n",
            "[Epoch 254/2200]  [Disc loss: -2.747740] [Gen loss: 0.182152] \n",
            "[Epoch 255/2200]  [Disc loss: -2.637736] [Gen loss: 0.830277] \n",
            "[Epoch 256/2200]  [Disc loss: -2.650179] [Gen loss: 1.287294] \n",
            "[Epoch 257/2200]  [Disc loss: -2.535461] [Gen loss: 1.308706] \n",
            "[Epoch 258/2200]  [Disc loss: -2.497066] [Gen loss: 1.198558] \n",
            "[Epoch 259/2200]  [Disc loss: -2.401879] [Gen loss: 1.077180] \n",
            "[Epoch 260/2200]  [Disc loss: -2.318964] [Gen loss: 1.507879] \n",
            "[Epoch 261/2200]  [Disc loss: -2.303543] [Gen loss: 1.496685] \n",
            "[Epoch 262/2200]  [Disc loss: -2.211269] [Gen loss: 1.147511] \n",
            "[Epoch 263/2200]  [Disc loss: -2.257532] [Gen loss: 1.120767] \n",
            "[Epoch 264/2200]  [Disc loss: -2.198907] [Gen loss: 1.133123] \n",
            "[Epoch 265/2200]  [Disc loss: -2.068275] [Gen loss: 0.679901] \n",
            "[Epoch 266/2200]  [Disc loss: -1.985042] [Gen loss: 0.872793] \n",
            "[Epoch 267/2200]  [Disc loss: -1.932076] [Gen loss: 1.052629] \n",
            "[Epoch 268/2200]  [Disc loss: -1.937424] [Gen loss: 1.340926] \n",
            "[Epoch 269/2200]  [Disc loss: -1.976456] [Gen loss: 1.144656] \n",
            "[Epoch 270/2200]  [Disc loss: -1.941551] [Gen loss: 1.156511] \n",
            "[Epoch 271/2200]  [Disc loss: -1.968282] [Gen loss: 0.960536] \n",
            "[Epoch 272/2200]  [Disc loss: -1.921433] [Gen loss: 0.854313] \n",
            "[Epoch 273/2200]  [Disc loss: -1.909383] [Gen loss: 0.981030] \n",
            "[Epoch 274/2200]  [Disc loss: -1.826516] [Gen loss: 0.732458] \n",
            "[Epoch 275/2200]  [Disc loss: -1.815717] [Gen loss: 0.609180] \n",
            "[Epoch 276/2200]  [Disc loss: -1.843957] [Gen loss: 0.319889] \n",
            "[Epoch 277/2200]  [Disc loss: -1.941851] [Gen loss: 0.242850] \n",
            "[Epoch 278/2200]  [Disc loss: -1.840467] [Gen loss: -0.177547] \n",
            "[Epoch 279/2200]  [Disc loss: -1.731236] [Gen loss: -0.083945] \n",
            "[Epoch 280/2200]  [Disc loss: -1.752822] [Gen loss: -0.018114] \n",
            "[Epoch 281/2200]  [Disc loss: -1.688568] [Gen loss: -0.023014] \n",
            "[Epoch 282/2200]  [Disc loss: -1.819399] [Gen loss: -0.203845] \n",
            "[Epoch 283/2200]  [Disc loss: -1.796953] [Gen loss: -0.627702] \n",
            "[Epoch 284/2200]  [Disc loss: -1.755891] [Gen loss: -0.326968] \n",
            "[Epoch 285/2200]  [Disc loss: -1.681689] [Gen loss: -0.303226] \n",
            "[Epoch 286/2200]  [Disc loss: -1.718951] [Gen loss: -0.426176] \n",
            "[Epoch 287/2200]  [Disc loss: -1.740280] [Gen loss: -0.899344] \n",
            "[Epoch 288/2200]  [Disc loss: -1.791575] [Gen loss: -0.816408] \n",
            "[Epoch 289/2200]  [Disc loss: -1.793939] [Gen loss: -1.243540] \n",
            "[Epoch 290/2200]  [Disc loss: -1.703684] [Gen loss: -1.243543] \n",
            "[Epoch 291/2200]  [Disc loss: -1.702419] [Gen loss: -1.655199] \n",
            "[Epoch 292/2200]  [Disc loss: -1.704014] [Gen loss: -1.611693] \n",
            "[Epoch 293/2200]  [Disc loss: -1.662315] [Gen loss: -1.663146] \n",
            "[Epoch 294/2200]  [Disc loss: -1.696413] [Gen loss: -1.979790] \n",
            "[Epoch 295/2200]  [Disc loss: -1.673304] [Gen loss: -2.043141] \n",
            "[Epoch 296/2200]  [Disc loss: -1.639915] [Gen loss: -2.118345] \n",
            "[Epoch 297/2200]  [Disc loss: -1.688391] [Gen loss: -2.659922] \n",
            "[Epoch 298/2200]  [Disc loss: -1.622955] [Gen loss: -2.853716] \n",
            "[Epoch 299/2200]  [Disc loss: -1.572950] [Gen loss: -3.128155] \n",
            "[Epoch 300/2200]  [Disc loss: -1.566193] [Gen loss: -3.658693] \n",
            "[Epoch 301/2200]  [Disc loss: -1.491941] [Gen loss: -3.442438] \n",
            "[Epoch 302/2200]  [Disc loss: -1.516764] [Gen loss: -3.569192] \n",
            "[Epoch 303/2200]  [Disc loss: -1.524392] [Gen loss: -3.694869] \n",
            "[Epoch 304/2200]  [Disc loss: -1.501938] [Gen loss: -3.712918] \n",
            "[Epoch 305/2200]  [Disc loss: -1.513721] [Gen loss: -3.960139] \n",
            "[Epoch 306/2200]  [Disc loss: -1.551051] [Gen loss: -4.128880] \n",
            "[Epoch 307/2200]  [Disc loss: -1.526078] [Gen loss: -4.289722] \n",
            "[Epoch 308/2200]  [Disc loss: -1.514038] [Gen loss: -4.346810] \n",
            "[Epoch 309/2200]  [Disc loss: -1.521941] [Gen loss: -4.293392] \n",
            "[Epoch 310/2200]  [Disc loss: -1.446084] [Gen loss: -4.232822] \n",
            "[Epoch 311/2200]  [Disc loss: -1.429348] [Gen loss: -4.397637] \n",
            "[Epoch 312/2200]  [Disc loss: -1.387050] [Gen loss: -4.588672] \n",
            "[Epoch 313/2200]  [Disc loss: -1.373814] [Gen loss: -4.671765] \n",
            "[Epoch 314/2200]  [Disc loss: -1.374680] [Gen loss: -4.840564] \n",
            "[Epoch 315/2200]  [Disc loss: -1.422158] [Gen loss: -5.168292] \n",
            "[Epoch 316/2200]  [Disc loss: -1.433068] [Gen loss: -5.086351] \n",
            "[Epoch 317/2200]  [Disc loss: -1.416262] [Gen loss: -4.955817] \n",
            "[Epoch 318/2200]  [Disc loss: -1.387504] [Gen loss: -5.074335] \n",
            "[Epoch 319/2200]  [Disc loss: -1.361992] [Gen loss: -5.100768] \n",
            "[Epoch 320/2200]  [Disc loss: -1.364667] [Gen loss: -5.200928] \n",
            "[Epoch 321/2200]  [Disc loss: -1.347440] [Gen loss: -5.175721] \n",
            "[Epoch 322/2200]  [Disc loss: -1.309388] [Gen loss: -4.982399] \n",
            "[Epoch 323/2200]  [Disc loss: -1.333518] [Gen loss: -4.685997] \n",
            "[Epoch 324/2200]  [Disc loss: -1.342177] [Gen loss: -4.498147] \n",
            "[Epoch 325/2200]  [Disc loss: -1.322499] [Gen loss: -4.856684] \n",
            "[Epoch 326/2200]  [Disc loss: -1.310046] [Gen loss: -4.404205] \n",
            "[Epoch 327/2200]  [Disc loss: -1.270514] [Gen loss: -4.531365] \n",
            "[Epoch 328/2200]  [Disc loss: -1.280817] [Gen loss: -4.527577] \n",
            "[Epoch 329/2200]  [Disc loss: -1.276020] [Gen loss: -4.586621] \n",
            "[Epoch 330/2200]  [Disc loss: -1.260365] [Gen loss: -4.430836] \n",
            "[Epoch 331/2200]  [Disc loss: -1.229136] [Gen loss: -4.443196] \n",
            "[Epoch 332/2200]  [Disc loss: -1.250140] [Gen loss: -4.407587] \n",
            "[Epoch 333/2200]  [Disc loss: -1.255044] [Gen loss: -4.334491] \n",
            "[Epoch 334/2200]  [Disc loss: -1.215114] [Gen loss: -4.330167] \n",
            "[Epoch 335/2200]  [Disc loss: -1.228758] [Gen loss: -4.029363] \n",
            "[Epoch 336/2200]  [Disc loss: -1.200103] [Gen loss: -4.051472] \n",
            "[Epoch 337/2200]  [Disc loss: -1.195324] [Gen loss: -3.921735] \n",
            "[Epoch 338/2200]  [Disc loss: -1.190208] [Gen loss: -4.108174] \n",
            "[Epoch 339/2200]  [Disc loss: -1.161266] [Gen loss: -3.837571] \n",
            "[Epoch 340/2200]  [Disc loss: -1.163637] [Gen loss: -4.168358] \n",
            "[Epoch 341/2200]  [Disc loss: -1.152795] [Gen loss: -4.073507] \n",
            "[Epoch 342/2200]  [Disc loss: -1.138790] [Gen loss: -4.077219] \n",
            "[Epoch 343/2200]  [Disc loss: -1.101112] [Gen loss: -3.802134] \n",
            "[Epoch 344/2200]  [Disc loss: -1.122136] [Gen loss: -3.774379] \n",
            "[Epoch 345/2200]  [Disc loss: -1.110178] [Gen loss: -3.819111] \n",
            "[Epoch 346/2200]  [Disc loss: -1.104577] [Gen loss: -3.841465] \n",
            "[Epoch 347/2200]  [Disc loss: -1.092772] [Gen loss: -3.424379] \n",
            "[Epoch 348/2200]  [Disc loss: -1.084623] [Gen loss: -3.819793] \n",
            "[Epoch 349/2200]  [Disc loss: -1.074420] [Gen loss: -3.408955] \n",
            "[Epoch 350/2200]  [Disc loss: -1.072435] [Gen loss: -3.465896] \n",
            "[Epoch 351/2200]  [Disc loss: -1.069377] [Gen loss: -3.504648] \n",
            "[Epoch 352/2200]  [Disc loss: -1.065971] [Gen loss: -3.307632] \n",
            "[Epoch 353/2200]  [Disc loss: -1.048212] [Gen loss: -3.509270] \n",
            "[Epoch 354/2200]  [Disc loss: -1.044946] [Gen loss: -3.855411] \n",
            "[Epoch 355/2200]  [Disc loss: -1.026497] [Gen loss: -3.405024] \n",
            "[Epoch 356/2200]  [Disc loss: -1.029200] [Gen loss: -3.146929] \n",
            "[Epoch 357/2200]  [Disc loss: -1.005153] [Gen loss: -3.295963] \n",
            "[Epoch 358/2200]  [Disc loss: -1.007252] [Gen loss: -3.367899] \n",
            "[Epoch 359/2200]  [Disc loss: -0.991993] [Gen loss: -3.118472] \n",
            "[Epoch 360/2200]  [Disc loss: -0.984504] [Gen loss: -3.037322] \n",
            "[Epoch 361/2200]  [Disc loss: -0.969356] [Gen loss: -3.300383] \n",
            "[Epoch 362/2200]  [Disc loss: -0.977698] [Gen loss: -3.117788] \n",
            "[Epoch 363/2200]  [Disc loss: -0.952167] [Gen loss: -3.087108] \n",
            "[Epoch 364/2200]  [Disc loss: -0.951162] [Gen loss: -3.008841] \n",
            "[Epoch 365/2200]  [Disc loss: -0.944107] [Gen loss: -2.699324] \n",
            "[Epoch 366/2200]  [Disc loss: -0.935090] [Gen loss: -2.973713] \n",
            "[Epoch 367/2200]  [Disc loss: -0.941787] [Gen loss: -2.982509] \n",
            "[Epoch 368/2200]  [Disc loss: -0.909521] [Gen loss: -2.927888] \n",
            "[Epoch 369/2200]  [Disc loss: -0.920121] [Gen loss: -2.608974] \n",
            "[Epoch 370/2200]  [Disc loss: -0.908981] [Gen loss: -2.582179] \n",
            "[Epoch 371/2200]  [Disc loss: -0.901117] [Gen loss: -2.567744] \n",
            "[Epoch 372/2200]  [Disc loss: -0.891893] [Gen loss: -2.681151] \n",
            "[Epoch 373/2200]  [Disc loss: -0.885372] [Gen loss: -2.635534] \n",
            "[Epoch 374/2200]  [Disc loss: -0.879748] [Gen loss: -2.963879] \n",
            "[Epoch 375/2200]  [Disc loss: -0.893062] [Gen loss: -2.831360] \n",
            "[Epoch 376/2200]  [Disc loss: -0.874580] [Gen loss: -2.669862] \n",
            "[Epoch 377/2200]  [Disc loss: -0.860998] [Gen loss: -2.389403] \n",
            "[Epoch 378/2200]  [Disc loss: -0.859688] [Gen loss: -2.671691] \n",
            "[Epoch 379/2200]  [Disc loss: -0.864611] [Gen loss: -2.592614] \n",
            "[Epoch 380/2200]  [Disc loss: -0.848549] [Gen loss: -2.740215] \n",
            "[Epoch 381/2200]  [Disc loss: -0.855337] [Gen loss: -2.763658] \n",
            "[Epoch 382/2200]  [Disc loss: -0.841731] [Gen loss: -2.467003] \n",
            "[Epoch 383/2200]  [Disc loss: -0.836354] [Gen loss: -2.649811] \n",
            "[Epoch 384/2200]  [Disc loss: -0.850290] [Gen loss: -2.749975] \n",
            "[Epoch 385/2200]  [Disc loss: -0.829359] [Gen loss: -2.459538] \n",
            "[Epoch 386/2200]  [Disc loss: -0.836854] [Gen loss: -2.547627] \n",
            "[Epoch 387/2200]  [Disc loss: -0.837234] [Gen loss: -2.500879] \n",
            "[Epoch 388/2200]  [Disc loss: -0.818633] [Gen loss: -2.836128] \n",
            "[Epoch 389/2200]  [Disc loss: -0.834794] [Gen loss: -2.501917] \n",
            "[Epoch 390/2200]  [Disc loss: -0.807582] [Gen loss: -2.496119] \n",
            "[Epoch 391/2200]  [Disc loss: -0.824904] [Gen loss: -2.427961] \n",
            "[Epoch 392/2200]  [Disc loss: -0.837709] [Gen loss: -2.848602] \n",
            "[Epoch 393/2200]  [Disc loss: -0.838751] [Gen loss: -2.527502] \n",
            "[Epoch 394/2200]  [Disc loss: -0.817339] [Gen loss: -2.239550] \n",
            "[Epoch 395/2200]  [Disc loss: -0.810167] [Gen loss: -2.303991] \n",
            "[Epoch 396/2200]  [Disc loss: -0.821140] [Gen loss: -2.639476] \n",
            "[Epoch 397/2200]  [Disc loss: -0.808097] [Gen loss: -2.145336] \n",
            "[Epoch 398/2200]  [Disc loss: -0.828246] [Gen loss: -2.245388] \n",
            "[Epoch 399/2200]  [Disc loss: -0.823409] [Gen loss: -2.342989] \n",
            "[Epoch 400/2200]  [Disc loss: -0.832846] [Gen loss: -2.311007] \n",
            "[Epoch 401/2200]  [Disc loss: -0.824820] [Gen loss: -2.075441] \n",
            "[Epoch 402/2200]  [Disc loss: -0.826607] [Gen loss: -2.476459] \n",
            "[Epoch 403/2200]  [Disc loss: -0.823277] [Gen loss: -2.074825] \n",
            "[Epoch 404/2200]  [Disc loss: -0.823688] [Gen loss: -2.228415] \n",
            "[Epoch 405/2200]  [Disc loss: -0.795553] [Gen loss: -2.369539] \n",
            "[Epoch 406/2200]  [Disc loss: -0.817262] [Gen loss: -2.151992] \n",
            "[Epoch 407/2200]  [Disc loss: -0.822119] [Gen loss: -2.062857] \n",
            "[Epoch 408/2200]  [Disc loss: -0.819470] [Gen loss: -1.697990] \n",
            "[Epoch 409/2200]  [Disc loss: -0.824053] [Gen loss: -1.770488] \n",
            "[Epoch 410/2200]  [Disc loss: -0.825224] [Gen loss: -1.518713] \n",
            "[Epoch 411/2200]  [Disc loss: -0.825335] [Gen loss: -1.537574] \n",
            "[Epoch 412/2200]  [Disc loss: -0.823908] [Gen loss: -1.606377] \n",
            "[Epoch 413/2200]  [Disc loss: -0.813928] [Gen loss: -1.764885] \n",
            "[Epoch 414/2200]  [Disc loss: -0.820696] [Gen loss: -1.844366] \n",
            "[Epoch 415/2200]  [Disc loss: -0.817220] [Gen loss: -1.952701] \n",
            "[Epoch 416/2200]  [Disc loss: -0.823976] [Gen loss: -1.611307] \n",
            "[Epoch 417/2200]  [Disc loss: -0.818498] [Gen loss: -1.793460] \n",
            "[Epoch 418/2200]  [Disc loss: -0.796674] [Gen loss: -1.612708] \n",
            "[Epoch 419/2200]  [Disc loss: -0.806508] [Gen loss: -1.780335] \n",
            "[Epoch 420/2200]  [Disc loss: -0.808009] [Gen loss: -1.772609] \n",
            "[Epoch 421/2200]  [Disc loss: -0.814088] [Gen loss: -1.592527] \n",
            "[Epoch 422/2200]  [Disc loss: -0.818590] [Gen loss: -1.432064] \n",
            "[Epoch 423/2200]  [Disc loss: -0.828364] [Gen loss: -1.528806] \n",
            "[Epoch 424/2200]  [Disc loss: -0.821622] [Gen loss: -1.494337] \n",
            "[Epoch 425/2200]  [Disc loss: -0.818598] [Gen loss: -1.249477] \n",
            "[Epoch 426/2200]  [Disc loss: -0.815542] [Gen loss: -1.279876] \n",
            "[Epoch 427/2200]  [Disc loss: -0.816349] [Gen loss: -1.480220] \n",
            "[Epoch 428/2200]  [Disc loss: -0.820743] [Gen loss: -1.395891] \n",
            "[Epoch 429/2200]  [Disc loss: -0.833733] [Gen loss: -1.567581] \n",
            "[Epoch 430/2200]  [Disc loss: -0.829782] [Gen loss: -1.221283] \n",
            "[Epoch 431/2200]  [Disc loss: -0.802914] [Gen loss: -1.096634] \n",
            "[Epoch 432/2200]  [Disc loss: -0.827347] [Gen loss: -1.218699] \n",
            "[Epoch 433/2200]  [Disc loss: -0.804905] [Gen loss: -1.227813] \n",
            "[Epoch 434/2200]  [Disc loss: -0.823692] [Gen loss: -1.482976] \n",
            "[Epoch 435/2200]  [Disc loss: -0.827412] [Gen loss: -1.413378] \n",
            "[Epoch 436/2200]  [Disc loss: -0.825503] [Gen loss: -1.291666] \n",
            "[Epoch 437/2200]  [Disc loss: -0.820983] [Gen loss: -1.526166] \n",
            "[Epoch 438/2200]  [Disc loss: -0.818548] [Gen loss: -1.383940] \n",
            "[Epoch 439/2200]  [Disc loss: -0.832288] [Gen loss: -1.423372] \n",
            "[Epoch 440/2200]  [Disc loss: -0.833644] [Gen loss: -1.389178] \n",
            "[Epoch 441/2200]  [Disc loss: -0.824986] [Gen loss: -1.533628] \n",
            "[Epoch 442/2200]  [Disc loss: -0.803505] [Gen loss: -1.501200] \n",
            "[Epoch 443/2200]  [Disc loss: -0.822578] [Gen loss: -1.497213] \n",
            "[Epoch 444/2200]  [Disc loss: -0.836247] [Gen loss: -1.672759] \n",
            "[Epoch 445/2200]  [Disc loss: -0.820425] [Gen loss: -1.672601] \n",
            "[Epoch 446/2200]  [Disc loss: -0.820749] [Gen loss: -1.618701] \n",
            "[Epoch 447/2200]  [Disc loss: -0.818050] [Gen loss: -1.446306] \n",
            "[Epoch 448/2200]  [Disc loss: -0.835135] [Gen loss: -1.493621] \n",
            "[Epoch 449/2200]  [Disc loss: -0.828219] [Gen loss: -1.478759] \n",
            "[Epoch 450/2200]  [Disc loss: -0.831511] [Gen loss: -1.643534] \n",
            "[Epoch 451/2200]  [Disc loss: -0.826069] [Gen loss: -1.663119] \n",
            "[Epoch 452/2200]  [Disc loss: -0.838729] [Gen loss: -1.754108] \n",
            "[Epoch 453/2200]  [Disc loss: -0.831636] [Gen loss: -1.665094] \n",
            "[Epoch 454/2200]  [Disc loss: -0.834038] [Gen loss: -1.737509] \n",
            "[Epoch 455/2200]  [Disc loss: -0.835726] [Gen loss: -1.592845] \n",
            "[Epoch 456/2200]  [Disc loss: -0.838243] [Gen loss: -1.536121] \n",
            "[Epoch 457/2200]  [Disc loss: -0.838204] [Gen loss: -1.698057] \n",
            "[Epoch 458/2200]  [Disc loss: -0.823130] [Gen loss: -1.346567] \n",
            "[Epoch 459/2200]  [Disc loss: -0.832203] [Gen loss: -1.552347] \n",
            "[Epoch 460/2200]  [Disc loss: -0.822989] [Gen loss: -1.487147] \n",
            "[Epoch 461/2200]  [Disc loss: -0.818036] [Gen loss: -1.514173] \n",
            "[Epoch 462/2200]  [Disc loss: -0.819561] [Gen loss: -1.350513] \n",
            "[Epoch 463/2200]  [Disc loss: -0.833391] [Gen loss: -1.368564] \n",
            "[Epoch 464/2200]  [Disc loss: -0.825165] [Gen loss: -1.571428] \n",
            "[Epoch 465/2200]  [Disc loss: -0.804409] [Gen loss: -1.071022] \n",
            "[Epoch 466/2200]  [Disc loss: -0.827138] [Gen loss: -1.332018] \n",
            "[Epoch 467/2200]  [Disc loss: -0.829010] [Gen loss: -1.241086] \n",
            "[Epoch 468/2200]  [Disc loss: -0.815932] [Gen loss: -1.280270] \n",
            "[Epoch 469/2200]  [Disc loss: -0.825365] [Gen loss: -1.460167] \n",
            "[Epoch 470/2200]  [Disc loss: -0.816865] [Gen loss: -1.145733] \n",
            "[Epoch 471/2200]  [Disc loss: -0.810981] [Gen loss: -1.367886] \n",
            "[Epoch 472/2200]  [Disc loss: -0.817054] [Gen loss: -1.380276] \n",
            "[Epoch 473/2200]  [Disc loss: -0.805953] [Gen loss: -1.335856] \n",
            "[Epoch 474/2200]  [Disc loss: -0.817744] [Gen loss: -1.236178] \n",
            "[Epoch 475/2200]  [Disc loss: -0.804573] [Gen loss: -1.337008] \n",
            "[Epoch 476/2200]  [Disc loss: -0.802678] [Gen loss: -1.285317] \n",
            "[Epoch 477/2200]  [Disc loss: -0.813109] [Gen loss: -1.173015] \n",
            "[Epoch 478/2200]  [Disc loss: -0.796013] [Gen loss: -1.127179] \n",
            "[Epoch 479/2200]  [Disc loss: -0.810877] [Gen loss: -1.129511] \n",
            "[Epoch 480/2200]  [Disc loss: -0.792252] [Gen loss: -1.224224] \n",
            "[Epoch 481/2200]  [Disc loss: -0.803120] [Gen loss: -1.142106] \n",
            "[Epoch 482/2200]  [Disc loss: -0.795821] [Gen loss: -1.225928] \n",
            "[Epoch 483/2200]  [Disc loss: -0.798514] [Gen loss: -1.059758] \n",
            "[Epoch 484/2200]  [Disc loss: -0.799865] [Gen loss: -1.288540] \n",
            "[Epoch 485/2200]  [Disc loss: -0.791534] [Gen loss: -1.037686] \n",
            "[Epoch 486/2200]  [Disc loss: -0.791363] [Gen loss: -1.078362] \n",
            "[Epoch 487/2200]  [Disc loss: -0.793754] [Gen loss: -0.793979] \n",
            "[Epoch 488/2200]  [Disc loss: -0.789410] [Gen loss: -0.911032] \n",
            "[Epoch 489/2200]  [Disc loss: -0.784522] [Gen loss: -0.996996] \n",
            "[Epoch 490/2200]  [Disc loss: -0.788312] [Gen loss: -0.939743] \n",
            "[Epoch 491/2200]  [Disc loss: -0.782592] [Gen loss: -0.905510] \n",
            "[Epoch 492/2200]  [Disc loss: -0.799820] [Gen loss: -1.061881] \n",
            "[Epoch 493/2200]  [Disc loss: -0.773269] [Gen loss: -0.766492] \n",
            "[Epoch 494/2200]  [Disc loss: -0.783265] [Gen loss: -0.588051] \n",
            "[Epoch 495/2200]  [Disc loss: -0.774678] [Gen loss: -0.599555] \n",
            "[Epoch 496/2200]  [Disc loss: -0.776539] [Gen loss: -0.570309] \n",
            "[Epoch 497/2200]  [Disc loss: -0.785334] [Gen loss: -0.756488] \n",
            "[Epoch 498/2200]  [Disc loss: -0.774489] [Gen loss: -0.389916] \n",
            "[Epoch 499/2200]  [Disc loss: -0.776400] [Gen loss: -0.379551] \n",
            "[Epoch 500/2200]  [Disc loss: -0.768148] [Gen loss: -0.265743] \n",
            "[Epoch 501/2200]  [Disc loss: -0.780922] [Gen loss: -0.270625] \n",
            "[Epoch 502/2200]  [Disc loss: -0.775438] [Gen loss: -0.329315] \n",
            "[Epoch 503/2200]  [Disc loss: -0.759932] [Gen loss: -0.179925] \n",
            "[Epoch 504/2200]  [Disc loss: -0.766441] [Gen loss: -0.153211] \n",
            "[Epoch 505/2200]  [Disc loss: -0.771429] [Gen loss: -0.384034] \n",
            "[Epoch 506/2200]  [Disc loss: -0.765286] [Gen loss: -0.205484] \n",
            "[Epoch 507/2200]  [Disc loss: -0.767552] [Gen loss: -0.039072] \n",
            "[Epoch 508/2200]  [Disc loss: -0.773129] [Gen loss: 0.129722] \n",
            "[Epoch 509/2200]  [Disc loss: -0.760854] [Gen loss: -0.053491] \n",
            "[Epoch 510/2200]  [Disc loss: -0.758060] [Gen loss: 0.201179] \n",
            "[Epoch 511/2200]  [Disc loss: -0.763878] [Gen loss: 0.016165] \n",
            "[Epoch 512/2200]  [Disc loss: -0.761093] [Gen loss: -0.048001] \n",
            "[Epoch 513/2200]  [Disc loss: -0.759431] [Gen loss: 0.229217] \n",
            "[Epoch 514/2200]  [Disc loss: -0.757967] [Gen loss: 0.392639] \n",
            "[Epoch 515/2200]  [Disc loss: -0.759755] [Gen loss: 0.187658] \n",
            "[Epoch 516/2200]  [Disc loss: -0.759461] [Gen loss: 0.243622] \n",
            "[Epoch 517/2200]  [Disc loss: -0.762528] [Gen loss: 0.524230] \n",
            "[Epoch 518/2200]  [Disc loss: -0.756341] [Gen loss: 0.370916] \n",
            "[Epoch 519/2200]  [Disc loss: -0.756488] [Gen loss: 0.578812] \n",
            "[Epoch 520/2200]  [Disc loss: -0.770162] [Gen loss: 0.529597] \n",
            "[Epoch 521/2200]  [Disc loss: -0.754033] [Gen loss: 0.755539] \n",
            "[Epoch 522/2200]  [Disc loss: -0.754852] [Gen loss: 0.747610] \n",
            "[Epoch 523/2200]  [Disc loss: -0.748728] [Gen loss: 0.680489] \n",
            "[Epoch 524/2200]  [Disc loss: -0.748318] [Gen loss: 0.904822] \n",
            "[Epoch 525/2200]  [Disc loss: -0.763740] [Gen loss: 0.732898] \n",
            "[Epoch 526/2200]  [Disc loss: -0.749974] [Gen loss: 0.812757] \n",
            "[Epoch 527/2200]  [Disc loss: -0.753341] [Gen loss: 0.738265] \n",
            "[Epoch 528/2200]  [Disc loss: -0.748641] [Gen loss: 1.062800] \n",
            "[Epoch 529/2200]  [Disc loss: -0.752687] [Gen loss: 0.919534] \n",
            "[Epoch 530/2200]  [Disc loss: -0.755549] [Gen loss: 0.978567] \n",
            "[Epoch 531/2200]  [Disc loss: -0.743763] [Gen loss: 1.000889] \n",
            "[Epoch 532/2200]  [Disc loss: -0.751441] [Gen loss: 1.233049] \n",
            "[Epoch 533/2200]  [Disc loss: -0.751529] [Gen loss: 1.067545] \n",
            "[Epoch 534/2200]  [Disc loss: -0.742756] [Gen loss: 1.102067] \n",
            "[Epoch 535/2200]  [Disc loss: -0.756569] [Gen loss: 1.293265] \n",
            "[Epoch 536/2200]  [Disc loss: -0.744124] [Gen loss: 1.029816] \n",
            "[Epoch 537/2200]  [Disc loss: -0.738163] [Gen loss: 1.349131] \n",
            "[Epoch 538/2200]  [Disc loss: -0.749573] [Gen loss: 1.418231] \n",
            "[Epoch 539/2200]  [Disc loss: -0.752078] [Gen loss: 1.436919] \n",
            "[Epoch 540/2200]  [Disc loss: -0.744111] [Gen loss: 1.489065] \n",
            "[Epoch 541/2200]  [Disc loss: -0.748219] [Gen loss: 1.532228] \n",
            "[Epoch 542/2200]  [Disc loss: -0.742873] [Gen loss: 1.565326] \n",
            "[Epoch 543/2200]  [Disc loss: -0.739115] [Gen loss: 1.830625] \n",
            "[Epoch 544/2200]  [Disc loss: -0.739183] [Gen loss: 1.590131] \n",
            "[Epoch 545/2200]  [Disc loss: -0.737214] [Gen loss: 1.615922] \n",
            "[Epoch 546/2200]  [Disc loss: -0.745708] [Gen loss: 1.710002] \n",
            "[Epoch 547/2200]  [Disc loss: -0.744368] [Gen loss: 1.837503] \n",
            "[Epoch 548/2200]  [Disc loss: -0.730872] [Gen loss: 1.837494] \n",
            "[Epoch 549/2200]  [Disc loss: -0.740976] [Gen loss: 1.799195] \n",
            "[Epoch 550/2200]  [Disc loss: -0.736176] [Gen loss: 2.011450] \n",
            "[Epoch 551/2200]  [Disc loss: -0.742459] [Gen loss: 1.921789] \n",
            "[Epoch 552/2200]  [Disc loss: -0.736961] [Gen loss: 2.077532] \n",
            "[Epoch 553/2200]  [Disc loss: -0.740314] [Gen loss: 1.935221] \n",
            "[Epoch 554/2200]  [Disc loss: -0.736726] [Gen loss: 2.264963] \n",
            "[Epoch 555/2200]  [Disc loss: -0.739223] [Gen loss: 2.154356] \n",
            "[Epoch 556/2200]  [Disc loss: -0.737252] [Gen loss: 2.090648] \n",
            "[Epoch 557/2200]  [Disc loss: -0.737334] [Gen loss: 2.111667] \n",
            "[Epoch 558/2200]  [Disc loss: -0.730448] [Gen loss: 2.090948] \n",
            "[Epoch 559/2200]  [Disc loss: -0.733255] [Gen loss: 2.294806] \n",
            "[Epoch 560/2200]  [Disc loss: -0.736818] [Gen loss: 2.205024] \n",
            "[Epoch 561/2200]  [Disc loss: -0.729793] [Gen loss: 2.001240] \n",
            "[Epoch 562/2200]  [Disc loss: -0.730606] [Gen loss: 2.172125] \n",
            "[Epoch 563/2200]  [Disc loss: -0.734192] [Gen loss: 2.133560] \n",
            "[Epoch 564/2200]  [Disc loss: -0.727973] [Gen loss: 2.063974] \n",
            "[Epoch 565/2200]  [Disc loss: -0.724301] [Gen loss: 2.133815] \n",
            "[Epoch 566/2200]  [Disc loss: -0.726533] [Gen loss: 2.097303] \n",
            "[Epoch 567/2200]  [Disc loss: -0.720146] [Gen loss: 2.263879] \n",
            "[Epoch 568/2200]  [Disc loss: -0.723570] [Gen loss: 2.285886] \n",
            "[Epoch 569/2200]  [Disc loss: -0.728088] [Gen loss: 2.229420] \n",
            "[Epoch 570/2200]  [Disc loss: -0.723095] [Gen loss: 2.215694] \n",
            "[Epoch 571/2200]  [Disc loss: -0.724175] [Gen loss: 2.330273] \n",
            "[Epoch 572/2200]  [Disc loss: -0.728550] [Gen loss: 2.294064] \n",
            "[Epoch 573/2200]  [Disc loss: -0.714479] [Gen loss: 2.478621] \n",
            "[Epoch 574/2200]  [Disc loss: -0.719469] [Gen loss: 2.255154] \n",
            "[Epoch 575/2200]  [Disc loss: -0.723779] [Gen loss: 2.426186] \n",
            "[Epoch 576/2200]  [Disc loss: -0.713551] [Gen loss: 2.567688] \n",
            "[Epoch 577/2200]  [Disc loss: -0.718449] [Gen loss: 2.612807] \n",
            "[Epoch 578/2200]  [Disc loss: -0.714457] [Gen loss: 2.301268] \n",
            "[Epoch 579/2200]  [Disc loss: -0.716242] [Gen loss: 2.587933] \n",
            "[Epoch 580/2200]  [Disc loss: -0.716543] [Gen loss: 2.397465] \n",
            "[Epoch 581/2200]  [Disc loss: -0.710360] [Gen loss: 2.511847] \n",
            "[Epoch 582/2200]  [Disc loss: -0.715955] [Gen loss: 2.384679] \n",
            "[Epoch 583/2200]  [Disc loss: -0.712157] [Gen loss: 2.401055] \n",
            "[Epoch 584/2200]  [Disc loss: -0.708572] [Gen loss: 2.612587] \n",
            "[Epoch 585/2200]  [Disc loss: -0.707039] [Gen loss: 2.585385] \n",
            "[Epoch 586/2200]  [Disc loss: -0.717134] [Gen loss: 2.406567] \n",
            "[Epoch 587/2200]  [Disc loss: -0.704547] [Gen loss: 2.534497] \n",
            "[Epoch 588/2200]  [Disc loss: -0.710785] [Gen loss: 2.605945] \n",
            "[Epoch 589/2200]  [Disc loss: -0.705150] [Gen loss: 2.466588] \n",
            "[Epoch 590/2200]  [Disc loss: -0.703013] [Gen loss: 2.419451] \n",
            "[Epoch 591/2200]  [Disc loss: -0.704421] [Gen loss: 2.648960] \n",
            "[Epoch 592/2200]  [Disc loss: -0.702808] [Gen loss: 2.474509] \n",
            "[Epoch 593/2200]  [Disc loss: -0.705231] [Gen loss: 2.492649] \n",
            "[Epoch 594/2200]  [Disc loss: -0.712041] [Gen loss: 2.558961] \n",
            "[Epoch 595/2200]  [Disc loss: -0.701507] [Gen loss: 2.659326] \n",
            "[Epoch 596/2200]  [Disc loss: -0.709391] [Gen loss: 2.757900] \n",
            "[Epoch 597/2200]  [Disc loss: -0.702600] [Gen loss: 2.502431] \n",
            "[Epoch 598/2200]  [Disc loss: -0.701615] [Gen loss: 2.730468] \n",
            "[Epoch 599/2200]  [Disc loss: -0.703371] [Gen loss: 2.441603] \n",
            "[Epoch 600/2200]  [Disc loss: -0.702555] [Gen loss: 2.650876] \n",
            "[Epoch 601/2200]  [Disc loss: -0.701005] [Gen loss: 2.576160] \n",
            "[Epoch 602/2200]  [Disc loss: -0.707117] [Gen loss: 2.384127] \n",
            "[Epoch 603/2200]  [Disc loss: -0.692288] [Gen loss: 2.408761] \n",
            "[Epoch 604/2200]  [Disc loss: -0.699450] [Gen loss: 2.662318] \n",
            "[Epoch 605/2200]  [Disc loss: -0.706328] [Gen loss: 2.514139] \n",
            "[Epoch 606/2200]  [Disc loss: -0.693676] [Gen loss: 2.576729] \n",
            "[Epoch 607/2200]  [Disc loss: -0.697507] [Gen loss: 2.630642] \n",
            "[Epoch 608/2200]  [Disc loss: -0.703054] [Gen loss: 2.546436] \n",
            "[Epoch 609/2200]  [Disc loss: -0.701815] [Gen loss: 2.637831] \n",
            "[Epoch 610/2200]  [Disc loss: -0.701636] [Gen loss: 2.468956] \n",
            "[Epoch 611/2200]  [Disc loss: -0.700717] [Gen loss: 2.728492] \n",
            "[Epoch 612/2200]  [Disc loss: -0.703720] [Gen loss: 2.682989] \n",
            "[Epoch 613/2200]  [Disc loss: -0.699272] [Gen loss: 2.679105] \n",
            "[Epoch 614/2200]  [Disc loss: -0.697918] [Gen loss: 2.549998] \n",
            "[Epoch 615/2200]  [Disc loss: -0.696462] [Gen loss: 2.702975] \n",
            "[Epoch 616/2200]  [Disc loss: -0.700856] [Gen loss: 2.588302] \n",
            "[Epoch 617/2200]  [Disc loss: -0.695155] [Gen loss: 2.632350] \n",
            "[Epoch 618/2200]  [Disc loss: -0.696198] [Gen loss: 2.631972] \n",
            "[Epoch 619/2200]  [Disc loss: -0.697514] [Gen loss: 2.661170] \n",
            "[Epoch 620/2200]  [Disc loss: -0.697217] [Gen loss: 2.409304] \n",
            "[Epoch 621/2200]  [Disc loss: -0.702455] [Gen loss: 2.563832] \n",
            "[Epoch 622/2200]  [Disc loss: -0.695323] [Gen loss: 2.680182] \n",
            "[Epoch 623/2200]  [Disc loss: -0.690162] [Gen loss: 2.500806] \n",
            "[Epoch 624/2200]  [Disc loss: -0.694768] [Gen loss: 2.582284] \n",
            "[Epoch 625/2200]  [Disc loss: -0.696603] [Gen loss: 2.500105] \n",
            "[Epoch 626/2200]  [Disc loss: -0.698741] [Gen loss: 2.563338] \n",
            "[Epoch 627/2200]  [Disc loss: -0.688907] [Gen loss: 2.525167] \n",
            "[Epoch 628/2200]  [Disc loss: -0.690816] [Gen loss: 2.569791] \n",
            "[Epoch 629/2200]  [Disc loss: -0.694468] [Gen loss: 2.630482] \n",
            "[Epoch 630/2200]  [Disc loss: -0.691609] [Gen loss: 2.659444] \n",
            "[Epoch 631/2200]  [Disc loss: -0.690946] [Gen loss: 2.524413] \n",
            "[Epoch 632/2200]  [Disc loss: -0.694415] [Gen loss: 2.558736] \n",
            "[Epoch 633/2200]  [Disc loss: -0.690411] [Gen loss: 2.606745] \n",
            "[Epoch 634/2200]  [Disc loss: -0.696608] [Gen loss: 2.608481] \n",
            "[Epoch 635/2200]  [Disc loss: -0.690396] [Gen loss: 2.745299] \n",
            "[Epoch 636/2200]  [Disc loss: -0.685919] [Gen loss: 2.527660] \n",
            "[Epoch 637/2200]  [Disc loss: -0.695256] [Gen loss: 2.678278] \n",
            "[Epoch 638/2200]  [Disc loss: -0.695112] [Gen loss: 2.766417] \n",
            "[Epoch 639/2200]  [Disc loss: -0.691702] [Gen loss: 2.542970] \n",
            "[Epoch 640/2200]  [Disc loss: -0.688491] [Gen loss: 2.597271] \n",
            "[Epoch 641/2200]  [Disc loss: -0.685768] [Gen loss: 2.745403] \n",
            "[Epoch 642/2200]  [Disc loss: -0.693325] [Gen loss: 2.649741] \n",
            "[Epoch 643/2200]  [Disc loss: -0.684172] [Gen loss: 2.763941] \n",
            "[Epoch 644/2200]  [Disc loss: -0.694167] [Gen loss: 2.654984] \n",
            "[Epoch 645/2200]  [Disc loss: -0.690164] [Gen loss: 2.678713] \n",
            "[Epoch 646/2200]  [Disc loss: -0.686222] [Gen loss: 2.799130] \n",
            "[Epoch 647/2200]  [Disc loss: -0.692155] [Gen loss: 2.762336] \n",
            "[Epoch 648/2200]  [Disc loss: -0.689744] [Gen loss: 2.645545] \n",
            "[Epoch 649/2200]  [Disc loss: -0.688803] [Gen loss: 2.702299] \n",
            "[Epoch 650/2200]  [Disc loss: -0.687601] [Gen loss: 2.571638] \n",
            "[Epoch 651/2200]  [Disc loss: -0.694406] [Gen loss: 2.699853] \n",
            "[Epoch 652/2200]  [Disc loss: -0.692576] [Gen loss: 2.694856] \n",
            "[Epoch 653/2200]  [Disc loss: -0.687328] [Gen loss: 2.716950] \n",
            "[Epoch 654/2200]  [Disc loss: -0.683468] [Gen loss: 2.591871] \n",
            "[Epoch 655/2200]  [Disc loss: -0.688937] [Gen loss: 2.648178] \n",
            "[Epoch 656/2200]  [Disc loss: -0.686475] [Gen loss: 2.717277] \n",
            "[Epoch 657/2200]  [Disc loss: -0.689402] [Gen loss: 2.677516] \n",
            "[Epoch 658/2200]  [Disc loss: -0.684034] [Gen loss: 2.785620] \n",
            "[Epoch 659/2200]  [Disc loss: -0.687672] [Gen loss: 2.708038] \n",
            "[Epoch 660/2200]  [Disc loss: -0.688908] [Gen loss: 2.821154] \n",
            "[Epoch 661/2200]  [Disc loss: -0.688560] [Gen loss: 2.607615] \n",
            "[Epoch 662/2200]  [Disc loss: -0.690705] [Gen loss: 2.807008] \n",
            "[Epoch 663/2200]  [Disc loss: -0.687798] [Gen loss: 2.778181] \n",
            "[Epoch 664/2200]  [Disc loss: -0.689683] [Gen loss: 2.850885] \n",
            "[Epoch 665/2200]  [Disc loss: -0.685740] [Gen loss: 2.870724] \n",
            "[Epoch 666/2200]  [Disc loss: -0.688340] [Gen loss: 2.677562] \n",
            "[Epoch 667/2200]  [Disc loss: -0.686691] [Gen loss: 2.689330] \n",
            "[Epoch 668/2200]  [Disc loss: -0.685155] [Gen loss: 2.803170] \n",
            "[Epoch 669/2200]  [Disc loss: -0.689197] [Gen loss: 2.578746] \n",
            "[Epoch 670/2200]  [Disc loss: -0.687378] [Gen loss: 2.717985] \n",
            "[Epoch 671/2200]  [Disc loss: -0.680985] [Gen loss: 2.623370] \n",
            "[Epoch 672/2200]  [Disc loss: -0.687234] [Gen loss: 2.640068] \n",
            "[Epoch 673/2200]  [Disc loss: -0.686140] [Gen loss: 2.579213] \n",
            "[Epoch 674/2200]  [Disc loss: -0.686584] [Gen loss: 2.684074] \n",
            "[Epoch 675/2200]  [Disc loss: -0.687068] [Gen loss: 2.629491] \n",
            "[Epoch 676/2200]  [Disc loss: -0.686011] [Gen loss: 2.611159] \n",
            "[Epoch 677/2200]  [Disc loss: -0.683539] [Gen loss: 2.724376] \n",
            "[Epoch 678/2200]  [Disc loss: -0.683918] [Gen loss: 2.615957] \n",
            "[Epoch 679/2200]  [Disc loss: -0.681724] [Gen loss: 2.563634] \n",
            "[Epoch 680/2200]  [Disc loss: -0.686475] [Gen loss: 2.750651] \n",
            "[Epoch 681/2200]  [Disc loss: -0.687035] [Gen loss: 2.625403] \n",
            "[Epoch 682/2200]  [Disc loss: -0.689251] [Gen loss: 2.616834] \n",
            "[Epoch 683/2200]  [Disc loss: -0.685751] [Gen loss: 2.607412] \n",
            "[Epoch 684/2200]  [Disc loss: -0.685099] [Gen loss: 2.785857] \n",
            "[Epoch 685/2200]  [Disc loss: -0.684114] [Gen loss: 2.528871] \n",
            "[Epoch 686/2200]  [Disc loss: -0.680950] [Gen loss: 2.641200] \n",
            "[Epoch 687/2200]  [Disc loss: -0.685562] [Gen loss: 2.745627] \n",
            "[Epoch 688/2200]  [Disc loss: -0.685680] [Gen loss: 2.661720] \n",
            "[Epoch 689/2200]  [Disc loss: -0.679293] [Gen loss: 2.631378] \n",
            "[Epoch 690/2200]  [Disc loss: -0.685865] [Gen loss: 2.671915] \n",
            "[Epoch 691/2200]  [Disc loss: -0.683585] [Gen loss: 2.749605] \n",
            "[Epoch 692/2200]  [Disc loss: -0.681781] [Gen loss: 2.614844] \n",
            "[Epoch 693/2200]  [Disc loss: -0.685158] [Gen loss: 2.695518] \n",
            "[Epoch 694/2200]  [Disc loss: -0.682716] [Gen loss: 2.722651] \n",
            "[Epoch 695/2200]  [Disc loss: -0.684889] [Gen loss: 2.783605] \n",
            "[Epoch 696/2200]  [Disc loss: -0.683728] [Gen loss: 2.826931] \n",
            "[Epoch 697/2200]  [Disc loss: -0.685781] [Gen loss: 2.804042] \n",
            "[Epoch 698/2200]  [Disc loss: -0.681505] [Gen loss: 2.674049] \n",
            "[Epoch 699/2200]  [Disc loss: -0.681449] [Gen loss: 2.751063] \n",
            "[Epoch 700/2200]  [Disc loss: -0.684165] [Gen loss: 2.691831] \n",
            "[Epoch 701/2200]  [Disc loss: -0.680588] [Gen loss: 2.833680] \n",
            "[Epoch 702/2200]  [Disc loss: -0.683386] [Gen loss: 2.711511] \n",
            "[Epoch 703/2200]  [Disc loss: -0.678455] [Gen loss: 2.816742] \n",
            "[Epoch 704/2200]  [Disc loss: -0.682806] [Gen loss: 2.780990] \n",
            "[Epoch 705/2200]  [Disc loss: -0.684072] [Gen loss: 2.719328] \n",
            "[Epoch 706/2200]  [Disc loss: -0.674217] [Gen loss: 2.741059] \n",
            "[Epoch 707/2200]  [Disc loss: -0.684349] [Gen loss: 2.786214] \n",
            "[Epoch 708/2200]  [Disc loss: -0.677899] [Gen loss: 2.727099] \n",
            "[Epoch 709/2200]  [Disc loss: -0.677512] [Gen loss: 2.803509] \n",
            "[Epoch 710/2200]  [Disc loss: -0.686938] [Gen loss: 2.993296] \n",
            "[Epoch 711/2200]  [Disc loss: -0.678039] [Gen loss: 2.790662] \n",
            "[Epoch 712/2200]  [Disc loss: -0.675861] [Gen loss: 2.853614] \n",
            "[Epoch 713/2200]  [Disc loss: -0.678434] [Gen loss: 2.940523] \n",
            "[Epoch 714/2200]  [Disc loss: -0.677718] [Gen loss: 2.934776] \n",
            "[Epoch 715/2200]  [Disc loss: -0.677482] [Gen loss: 2.791306] \n",
            "[Epoch 716/2200]  [Disc loss: -0.677092] [Gen loss: 3.067458] \n",
            "[Epoch 717/2200]  [Disc loss: -0.676665] [Gen loss: 2.965228] \n",
            "[Epoch 718/2200]  [Disc loss: -0.680270] [Gen loss: 2.988926] \n",
            "[Epoch 719/2200]  [Disc loss: -0.680739] [Gen loss: 2.958516] \n",
            "[Epoch 720/2200]  [Disc loss: -0.670520] [Gen loss: 3.021599] \n",
            "[Epoch 721/2200]  [Disc loss: -0.681041] [Gen loss: 3.073771] \n",
            "[Epoch 722/2200]  [Disc loss: -0.679376] [Gen loss: 2.929974] \n",
            "[Epoch 723/2200]  [Disc loss: -0.678194] [Gen loss: 3.048053] \n",
            "[Epoch 724/2200]  [Disc loss: -0.678782] [Gen loss: 3.065888] \n",
            "[Epoch 725/2200]  [Disc loss: -0.668774] [Gen loss: 3.116100] \n",
            "[Epoch 726/2200]  [Disc loss: -0.676458] [Gen loss: 2.996125] \n",
            "[Epoch 727/2200]  [Disc loss: -0.672621] [Gen loss: 3.096416] \n",
            "[Epoch 728/2200]  [Disc loss: -0.672846] [Gen loss: 2.980255] \n",
            "[Epoch 729/2200]  [Disc loss: -0.679041] [Gen loss: 2.896809] \n",
            "[Epoch 730/2200]  [Disc loss: -0.670016] [Gen loss: 3.199441] \n",
            "[Epoch 731/2200]  [Disc loss: -0.670975] [Gen loss: 3.028498] \n",
            "[Epoch 732/2200]  [Disc loss: -0.674548] [Gen loss: 3.138055] \n",
            "[Epoch 733/2200]  [Disc loss: -0.669491] [Gen loss: 3.005129] \n",
            "[Epoch 734/2200]  [Disc loss: -0.665945] [Gen loss: 3.007581] \n",
            "[Epoch 735/2200]  [Disc loss: -0.665113] [Gen loss: 2.934980] \n",
            "[Epoch 736/2200]  [Disc loss: -0.669636] [Gen loss: 3.093103] \n",
            "[Epoch 737/2200]  [Disc loss: -0.666334] [Gen loss: 2.977670] \n",
            "[Epoch 738/2200]  [Disc loss: -0.665473] [Gen loss: 3.084011] \n",
            "[Epoch 739/2200]  [Disc loss: -0.666523] [Gen loss: 3.033624] \n",
            "[Epoch 740/2200]  [Disc loss: -0.669799] [Gen loss: 3.019297] \n",
            "[Epoch 741/2200]  [Disc loss: -0.669447] [Gen loss: 2.933229] \n",
            "[Epoch 742/2200]  [Disc loss: -0.664672] [Gen loss: 3.048397] \n",
            "[Epoch 743/2200]  [Disc loss: -0.664339] [Gen loss: 3.067168] \n",
            "[Epoch 744/2200]  [Disc loss: -0.667487] [Gen loss: 3.048028] \n",
            "[Epoch 745/2200]  [Disc loss: -0.668679] [Gen loss: 2.933341] \n",
            "[Epoch 746/2200]  [Disc loss: -0.666904] [Gen loss: 3.058561] \n",
            "[Epoch 747/2200]  [Disc loss: -0.667977] [Gen loss: 3.160193] \n",
            "[Epoch 748/2200]  [Disc loss: -0.664876] [Gen loss: 3.145092] \n",
            "[Epoch 749/2200]  [Disc loss: -0.666617] [Gen loss: 3.177554] \n",
            "[Epoch 750/2200]  [Disc loss: -0.664260] [Gen loss: 3.132717] \n",
            "[Epoch 751/2200]  [Disc loss: -0.663235] [Gen loss: 3.154366] \n",
            "[Epoch 752/2200]  [Disc loss: -0.664318] [Gen loss: 3.226736] \n",
            "[Epoch 753/2200]  [Disc loss: -0.665326] [Gen loss: 3.251962] \n",
            "[Epoch 754/2200]  [Disc loss: -0.662510] [Gen loss: 3.247209] \n",
            "[Epoch 755/2200]  [Disc loss: -0.664651] [Gen loss: 3.070396] \n",
            "[Epoch 756/2200]  [Disc loss: -0.661674] [Gen loss: 3.289530] \n",
            "[Epoch 757/2200]  [Disc loss: -0.663378] [Gen loss: 3.294384] \n",
            "[Epoch 758/2200]  [Disc loss: -0.665310] [Gen loss: 3.177635] \n",
            "[Epoch 759/2200]  [Disc loss: -0.656377] [Gen loss: 3.360086] \n",
            "[Epoch 760/2200]  [Disc loss: -0.664591] [Gen loss: 3.256728] \n",
            "[Epoch 761/2200]  [Disc loss: -0.662168] [Gen loss: 3.042872] \n",
            "[Epoch 762/2200]  [Disc loss: -0.660119] [Gen loss: 3.214141] \n",
            "[Epoch 763/2200]  [Disc loss: -0.668704] [Gen loss: 3.095658] \n",
            "[Epoch 764/2200]  [Disc loss: -0.659530] [Gen loss: 3.260570] \n",
            "[Epoch 765/2200]  [Disc loss: -0.665888] [Gen loss: 3.266137] \n",
            "[Epoch 766/2200]  [Disc loss: -0.658294] [Gen loss: 3.251590] \n",
            "[Epoch 767/2200]  [Disc loss: -0.660329] [Gen loss: 3.281611] \n",
            "[Epoch 768/2200]  [Disc loss: -0.659587] [Gen loss: 3.224344] \n",
            "[Epoch 769/2200]  [Disc loss: -0.658384] [Gen loss: 3.080275] \n",
            "[Epoch 770/2200]  [Disc loss: -0.655669] [Gen loss: 3.243045] \n",
            "[Epoch 771/2200]  [Disc loss: -0.660793] [Gen loss: 3.160669] \n",
            "[Epoch 772/2200]  [Disc loss: -0.655385] [Gen loss: 3.150409] \n",
            "[Epoch 773/2200]  [Disc loss: -0.659291] [Gen loss: 3.341874] \n",
            "[Epoch 774/2200]  [Disc loss: -0.652969] [Gen loss: 3.194442] \n",
            "[Epoch 775/2200]  [Disc loss: -0.663166] [Gen loss: 3.174751] \n",
            "[Epoch 776/2200]  [Disc loss: -0.657801] [Gen loss: 3.226039] \n",
            "[Epoch 777/2200]  [Disc loss: -0.662437] [Gen loss: 3.204545] \n",
            "[Epoch 778/2200]  [Disc loss: -0.653630] [Gen loss: 3.080324] \n",
            "[Epoch 779/2200]  [Disc loss: -0.656324] [Gen loss: 3.239173] \n",
            "[Epoch 780/2200]  [Disc loss: -0.656955] [Gen loss: 3.152937] \n",
            "[Epoch 781/2200]  [Disc loss: -0.656402] [Gen loss: 3.148910] \n",
            "[Epoch 782/2200]  [Disc loss: -0.657084] [Gen loss: 3.131319] \n",
            "[Epoch 783/2200]  [Disc loss: -0.653353] [Gen loss: 3.120254] \n",
            "[Epoch 784/2200]  [Disc loss: -0.653348] [Gen loss: 3.103370] \n",
            "[Epoch 785/2200]  [Disc loss: -0.654035] [Gen loss: 3.152228] \n",
            "[Epoch 786/2200]  [Disc loss: -0.662936] [Gen loss: 3.189250] \n",
            "[Epoch 787/2200]  [Disc loss: -0.653679] [Gen loss: 3.196308] \n",
            "[Epoch 788/2200]  [Disc loss: -0.657759] [Gen loss: 2.996545] \n",
            "[Epoch 789/2200]  [Disc loss: -0.657316] [Gen loss: 3.202708] \n",
            "[Epoch 790/2200]  [Disc loss: -0.656050] [Gen loss: 3.129745] \n",
            "[Epoch 791/2200]  [Disc loss: -0.661976] [Gen loss: 3.063016] \n",
            "[Epoch 792/2200]  [Disc loss: -0.657515] [Gen loss: 3.143677] \n",
            "[Epoch 793/2200]  [Disc loss: -0.655078] [Gen loss: 3.191892] \n",
            "[Epoch 794/2200]  [Disc loss: -0.657835] [Gen loss: 2.928184] \n",
            "[Epoch 795/2200]  [Disc loss: -0.657159] [Gen loss: 3.070616] \n",
            "[Epoch 796/2200]  [Disc loss: -0.657330] [Gen loss: 3.047780] \n",
            "[Epoch 797/2200]  [Disc loss: -0.658514] [Gen loss: 3.033702] \n",
            "[Epoch 798/2200]  [Disc loss: -0.655239] [Gen loss: 3.070727] \n",
            "[Epoch 799/2200]  [Disc loss: -0.656399] [Gen loss: 3.089864] \n",
            "[Epoch 800/2200]  [Disc loss: -0.650672] [Gen loss: 3.067156] \n",
            "[Epoch 801/2200]  [Disc loss: -0.656298] [Gen loss: 2.948575] \n",
            "[Epoch 802/2200]  [Disc loss: -0.652739] [Gen loss: 3.084502] \n",
            "[Epoch 803/2200]  [Disc loss: -0.651154] [Gen loss: 3.046030] \n",
            "[Epoch 804/2200]  [Disc loss: -0.651121] [Gen loss: 3.088076] \n",
            "[Epoch 805/2200]  [Disc loss: -0.659616] [Gen loss: 2.973895] \n",
            "[Epoch 806/2200]  [Disc loss: -0.653360] [Gen loss: 2.992984] \n",
            "[Epoch 807/2200]  [Disc loss: -0.656960] [Gen loss: 3.010665] \n",
            "[Epoch 808/2200]  [Disc loss: -0.650469] [Gen loss: 2.956198] \n",
            "[Epoch 809/2200]  [Disc loss: -0.652546] [Gen loss: 3.020015] \n",
            "[Epoch 810/2200]  [Disc loss: -0.652836] [Gen loss: 3.042105] \n",
            "[Epoch 811/2200]  [Disc loss: -0.652580] [Gen loss: 3.128342] \n",
            "[Epoch 812/2200]  [Disc loss: -0.654532] [Gen loss: 3.150434] \n",
            "[Epoch 813/2200]  [Disc loss: -0.652793] [Gen loss: 3.154025] \n",
            "[Epoch 814/2200]  [Disc loss: -0.652373] [Gen loss: 3.072048] \n",
            "[Epoch 815/2200]  [Disc loss: -0.653340] [Gen loss: 3.064827] \n",
            "[Epoch 816/2200]  [Disc loss: -0.656525] [Gen loss: 3.134211] \n",
            "[Epoch 817/2200]  [Disc loss: -0.652369] [Gen loss: 3.161249] \n",
            "[Epoch 818/2200]  [Disc loss: -0.649239] [Gen loss: 3.155756] \n",
            "[Epoch 819/2200]  [Disc loss: -0.656425] [Gen loss: 3.141808] \n",
            "[Epoch 820/2200]  [Disc loss: -0.651100] [Gen loss: 3.105283] \n",
            "[Epoch 821/2200]  [Disc loss: -0.655436] [Gen loss: 3.131437] \n",
            "[Epoch 822/2200]  [Disc loss: -0.653418] [Gen loss: 3.112047] \n",
            "[Epoch 823/2200]  [Disc loss: -0.649800] [Gen loss: 2.977585] \n",
            "[Epoch 824/2200]  [Disc loss: -0.649820] [Gen loss: 3.130225] \n",
            "[Epoch 825/2200]  [Disc loss: -0.649400] [Gen loss: 3.024076] \n",
            "[Epoch 826/2200]  [Disc loss: -0.657449] [Gen loss: 2.962532] \n",
            "[Epoch 827/2200]  [Disc loss: -0.649257] [Gen loss: 2.994649] \n",
            "[Epoch 828/2200]  [Disc loss: -0.652054] [Gen loss: 2.959811] \n",
            "[Epoch 829/2200]  [Disc loss: -0.649728] [Gen loss: 2.932610] \n",
            "[Epoch 830/2200]  [Disc loss: -0.646636] [Gen loss: 2.919136] \n",
            "[Epoch 831/2200]  [Disc loss: -0.655066] [Gen loss: 2.984262] \n",
            "[Epoch 832/2200]  [Disc loss: -0.647427] [Gen loss: 2.836706] \n",
            "[Epoch 833/2200]  [Disc loss: -0.647823] [Gen loss: 2.851472] \n",
            "[Epoch 834/2200]  [Disc loss: -0.649066] [Gen loss: 2.959339] \n",
            "[Epoch 835/2200]  [Disc loss: -0.650866] [Gen loss: 2.837546] \n",
            "[Epoch 836/2200]  [Disc loss: -0.646645] [Gen loss: 2.905886] \n",
            "[Epoch 837/2200]  [Disc loss: -0.648772] [Gen loss: 2.825327] \n",
            "[Epoch 838/2200]  [Disc loss: -0.655477] [Gen loss: 2.926321] \n",
            "[Epoch 839/2200]  [Disc loss: -0.649901] [Gen loss: 3.000581] \n",
            "[Epoch 840/2200]  [Disc loss: -0.652754] [Gen loss: 2.928086] \n",
            "[Epoch 841/2200]  [Disc loss: -0.646135] [Gen loss: 3.089688] \n",
            "[Epoch 842/2200]  [Disc loss: -0.646391] [Gen loss: 2.962007] \n",
            "[Epoch 843/2200]  [Disc loss: -0.645466] [Gen loss: 3.027551] \n",
            "[Epoch 844/2200]  [Disc loss: -0.654395] [Gen loss: 3.098908] \n",
            "[Epoch 845/2200]  [Disc loss: -0.646780] [Gen loss: 2.987501] \n",
            "[Epoch 846/2200]  [Disc loss: -0.650836] [Gen loss: 3.010804] \n",
            "[Epoch 847/2200]  [Disc loss: -0.645162] [Gen loss: 2.980551] \n",
            "[Epoch 848/2200]  [Disc loss: -0.648855] [Gen loss: 3.077336] \n",
            "[Epoch 849/2200]  [Disc loss: -0.646315] [Gen loss: 2.830092] \n",
            "[Epoch 850/2200]  [Disc loss: -0.644822] [Gen loss: 2.906570] \n",
            "[Epoch 851/2200]  [Disc loss: -0.650671] [Gen loss: 2.805167] \n",
            "[Epoch 852/2200]  [Disc loss: -0.648496] [Gen loss: 2.998890] \n",
            "[Epoch 853/2200]  [Disc loss: -0.642392] [Gen loss: 2.803145] \n",
            "[Epoch 854/2200]  [Disc loss: -0.644251] [Gen loss: 2.872574] \n",
            "[Epoch 855/2200]  [Disc loss: -0.645039] [Gen loss: 2.819170] \n",
            "[Epoch 856/2200]  [Disc loss: -0.642778] [Gen loss: 2.846768] \n",
            "[Epoch 857/2200]  [Disc loss: -0.644785] [Gen loss: 2.918143] \n",
            "[Epoch 858/2200]  [Disc loss: -0.649799] [Gen loss: 2.850675] \n",
            "[Epoch 859/2200]  [Disc loss: -0.646843] [Gen loss: 2.805548] \n",
            "[Epoch 860/2200]  [Disc loss: -0.644061] [Gen loss: 2.779872] \n",
            "[Epoch 861/2200]  [Disc loss: -0.651694] [Gen loss: 2.979078] \n",
            "[Epoch 862/2200]  [Disc loss: -0.647378] [Gen loss: 2.933384] \n",
            "[Epoch 863/2200]  [Disc loss: -0.644270] [Gen loss: 2.926933] \n",
            "[Epoch 864/2200]  [Disc loss: -0.646476] [Gen loss: 2.890666] \n",
            "[Epoch 865/2200]  [Disc loss: -0.648909] [Gen loss: 2.989607] \n",
            "[Epoch 866/2200]  [Disc loss: -0.645817] [Gen loss: 2.892714] \n",
            "[Epoch 867/2200]  [Disc loss: -0.644297] [Gen loss: 2.795539] \n",
            "[Epoch 868/2200]  [Disc loss: -0.644086] [Gen loss: 2.825515] \n",
            "[Epoch 869/2200]  [Disc loss: -0.645885] [Gen loss: 2.808969] \n",
            "[Epoch 870/2200]  [Disc loss: -0.647533] [Gen loss: 2.911614] \n",
            "[Epoch 871/2200]  [Disc loss: -0.649586] [Gen loss: 2.859834] \n",
            "[Epoch 872/2200]  [Disc loss: -0.643113] [Gen loss: 3.009529] \n",
            "[Epoch 873/2200]  [Disc loss: -0.641938] [Gen loss: 2.841092] \n",
            "[Epoch 874/2200]  [Disc loss: -0.643044] [Gen loss: 2.808566] \n",
            "[Epoch 875/2200]  [Disc loss: -0.648491] [Gen loss: 2.899516] \n",
            "[Epoch 876/2200]  [Disc loss: -0.647121] [Gen loss: 2.899955] \n",
            "[Epoch 877/2200]  [Disc loss: -0.637042] [Gen loss: 2.944131] \n",
            "[Epoch 878/2200]  [Disc loss: -0.646157] [Gen loss: 2.853618] \n",
            "[Epoch 879/2200]  [Disc loss: -0.642159] [Gen loss: 2.856113] \n",
            "[Epoch 880/2200]  [Disc loss: -0.642902] [Gen loss: 2.882758] \n",
            "[Epoch 881/2200]  [Disc loss: -0.639803] [Gen loss: 2.941910] \n",
            "[Epoch 882/2200]  [Disc loss: -0.642646] [Gen loss: 2.962038] \n",
            "[Epoch 883/2200]  [Disc loss: -0.639680] [Gen loss: 2.828018] \n",
            "[Epoch 884/2200]  [Disc loss: -0.644581] [Gen loss: 3.008583] \n",
            "[Epoch 885/2200]  [Disc loss: -0.635370] [Gen loss: 2.839626] \n",
            "[Epoch 886/2200]  [Disc loss: -0.645297] [Gen loss: 2.800465] \n",
            "[Epoch 887/2200]  [Disc loss: -0.640842] [Gen loss: 2.909003] \n",
            "[Epoch 888/2200]  [Disc loss: -0.638556] [Gen loss: 2.816802] \n",
            "[Epoch 889/2200]  [Disc loss: -0.637491] [Gen loss: 2.982788] \n",
            "[Epoch 890/2200]  [Disc loss: -0.646894] [Gen loss: 2.999759] \n",
            "[Epoch 891/2200]  [Disc loss: -0.638378] [Gen loss: 2.699210] \n",
            "[Epoch 892/2200]  [Disc loss: -0.641359] [Gen loss: 3.096343] \n",
            "[Epoch 893/2200]  [Disc loss: -0.642628] [Gen loss: 3.036849] \n",
            "[Epoch 894/2200]  [Disc loss: -0.642030] [Gen loss: 2.930583] \n",
            "[Epoch 895/2200]  [Disc loss: -0.635987] [Gen loss: 3.139570] \n",
            "[Epoch 896/2200]  [Disc loss: -0.644007] [Gen loss: 3.071554] \n",
            "[Epoch 897/2200]  [Disc loss: -0.645485] [Gen loss: 2.972986] \n",
            "[Epoch 898/2200]  [Disc loss: -0.638353] [Gen loss: 3.039171] \n",
            "[Epoch 899/2200]  [Disc loss: -0.644141] [Gen loss: 3.158591] \n",
            "[Epoch 900/2200]  [Disc loss: -0.643256] [Gen loss: 3.097411] \n",
            "[Epoch 901/2200]  [Disc loss: -0.640145] [Gen loss: 3.168576] \n",
            "[Epoch 902/2200]  [Disc loss: -0.642547] [Gen loss: 3.047453] \n",
            "[Epoch 903/2200]  [Disc loss: -0.640835] [Gen loss: 3.052936] \n",
            "[Epoch 904/2200]  [Disc loss: -0.635606] [Gen loss: 2.982773] \n",
            "[Epoch 905/2200]  [Disc loss: -0.640003] [Gen loss: 2.946552] \n",
            "[Epoch 906/2200]  [Disc loss: -0.641124] [Gen loss: 2.961450] \n",
            "[Epoch 907/2200]  [Disc loss: -0.633868] [Gen loss: 2.984600] \n",
            "[Epoch 908/2200]  [Disc loss: -0.641840] [Gen loss: 2.903669] \n",
            "[Epoch 909/2200]  [Disc loss: -0.638602] [Gen loss: 2.950850] \n",
            "[Epoch 910/2200]  [Disc loss: -0.638332] [Gen loss: 2.918709] \n",
            "[Epoch 911/2200]  [Disc loss: -0.636671] [Gen loss: 2.918631] \n",
            "[Epoch 912/2200]  [Disc loss: -0.638947] [Gen loss: 2.922199] \n",
            "[Epoch 913/2200]  [Disc loss: -0.631431] [Gen loss: 2.918047] \n",
            "[Epoch 914/2200]  [Disc loss: -0.637312] [Gen loss: 2.872249] \n",
            "[Epoch 915/2200]  [Disc loss: -0.635812] [Gen loss: 2.992428] \n",
            "[Epoch 916/2200]  [Disc loss: -0.635556] [Gen loss: 2.993642] \n",
            "[Epoch 917/2200]  [Disc loss: -0.635162] [Gen loss: 2.874771] \n",
            "[Epoch 918/2200]  [Disc loss: -0.634204] [Gen loss: 2.984085] \n",
            "[Epoch 919/2200]  [Disc loss: -0.635317] [Gen loss: 3.004933] \n",
            "[Epoch 920/2200]  [Disc loss: -0.640024] [Gen loss: 2.888369] \n",
            "[Epoch 921/2200]  [Disc loss: -0.638571] [Gen loss: 2.878655] \n",
            "[Epoch 922/2200]  [Disc loss: -0.642422] [Gen loss: 2.945065] \n",
            "[Epoch 923/2200]  [Disc loss: -0.637797] [Gen loss: 2.969183] \n",
            "[Epoch 924/2200]  [Disc loss: -0.638796] [Gen loss: 2.934435] \n",
            "[Epoch 925/2200]  [Disc loss: -0.639202] [Gen loss: 2.966134] \n",
            "[Epoch 926/2200]  [Disc loss: -0.636460] [Gen loss: 2.918295] \n",
            "[Epoch 927/2200]  [Disc loss: -0.632908] [Gen loss: 2.973919] \n",
            "[Epoch 928/2200]  [Disc loss: -0.638022] [Gen loss: 2.937977] \n",
            "[Epoch 929/2200]  [Disc loss: -0.637433] [Gen loss: 2.907931] \n",
            "[Epoch 930/2200]  [Disc loss: -0.634367] [Gen loss: 3.084087] \n",
            "[Epoch 931/2200]  [Disc loss: -0.637442] [Gen loss: 3.053013] \n",
            "[Epoch 932/2200]  [Disc loss: -0.634329] [Gen loss: 3.130816] \n",
            "[Epoch 933/2200]  [Disc loss: -0.637907] [Gen loss: 3.189616] \n",
            "[Epoch 934/2200]  [Disc loss: -0.631814] [Gen loss: 3.079822] \n",
            "[Epoch 935/2200]  [Disc loss: -0.630200] [Gen loss: 3.115262] \n",
            "[Epoch 936/2200]  [Disc loss: -0.631125] [Gen loss: 3.106508] \n",
            "[Epoch 937/2200]  [Disc loss: -0.631675] [Gen loss: 3.076415] \n",
            "[Epoch 938/2200]  [Disc loss: -0.632864] [Gen loss: 3.114945] \n",
            "[Epoch 939/2200]  [Disc loss: -0.639106] [Gen loss: 3.197426] \n",
            "[Epoch 940/2200]  [Disc loss: -0.631587] [Gen loss: 2.980661] \n",
            "[Epoch 941/2200]  [Disc loss: -0.633486] [Gen loss: 3.065994] \n",
            "[Epoch 942/2200]  [Disc loss: -0.640121] [Gen loss: 3.099867] \n",
            "[Epoch 943/2200]  [Disc loss: -0.633841] [Gen loss: 3.131600] \n",
            "[Epoch 944/2200]  [Disc loss: -0.630805] [Gen loss: 3.081495] \n",
            "[Epoch 945/2200]  [Disc loss: -0.633714] [Gen loss: 3.108140] \n",
            "[Epoch 946/2200]  [Disc loss: -0.631426] [Gen loss: 3.095635] \n",
            "[Epoch 947/2200]  [Disc loss: -0.634307] [Gen loss: 3.098918] \n",
            "[Epoch 948/2200]  [Disc loss: -0.637705] [Gen loss: 3.071177] \n",
            "[Epoch 949/2200]  [Disc loss: -0.629793] [Gen loss: 3.064748] \n",
            "[Epoch 950/2200]  [Disc loss: -0.632989] [Gen loss: 3.071853] \n",
            "[Epoch 951/2200]  [Disc loss: -0.631625] [Gen loss: 3.187018] \n",
            "[Epoch 952/2200]  [Disc loss: -0.635397] [Gen loss: 3.118162] \n",
            "[Epoch 953/2200]  [Disc loss: -0.627820] [Gen loss: 3.087192] \n",
            "[Epoch 954/2200]  [Disc loss: -0.625497] [Gen loss: 3.032474] \n",
            "[Epoch 955/2200]  [Disc loss: -0.630750] [Gen loss: 3.152171] \n",
            "[Epoch 956/2200]  [Disc loss: -0.630845] [Gen loss: 3.028479] \n",
            "[Epoch 957/2200]  [Disc loss: -0.629941] [Gen loss: 2.892657] \n",
            "[Epoch 958/2200]  [Disc loss: -0.632598] [Gen loss: 3.127461] \n",
            "[Epoch 959/2200]  [Disc loss: -0.628080] [Gen loss: 3.172151] \n",
            "[Epoch 960/2200]  [Disc loss: -0.628017] [Gen loss: 3.070911] \n",
            "[Epoch 961/2200]  [Disc loss: -0.631882] [Gen loss: 3.173305] \n",
            "[Epoch 962/2200]  [Disc loss: -0.628375] [Gen loss: 3.169971] \n",
            "[Epoch 963/2200]  [Disc loss: -0.633301] [Gen loss: 3.215585] \n",
            "[Epoch 964/2200]  [Disc loss: -0.627640] [Gen loss: 3.058067] \n",
            "[Epoch 965/2200]  [Disc loss: -0.631594] [Gen loss: 3.147072] \n",
            "[Epoch 966/2200]  [Disc loss: -0.626357] [Gen loss: 3.102194] \n",
            "[Epoch 967/2200]  [Disc loss: -0.626777] [Gen loss: 3.117200] \n",
            "[Epoch 968/2200]  [Disc loss: -0.628018] [Gen loss: 3.188826] \n",
            "[Epoch 969/2200]  [Disc loss: -0.631255] [Gen loss: 3.068297] \n",
            "[Epoch 970/2200]  [Disc loss: -0.625716] [Gen loss: 3.141851] \n",
            "[Epoch 971/2200]  [Disc loss: -0.628990] [Gen loss: 3.115294] \n",
            "[Epoch 972/2200]  [Disc loss: -0.622011] [Gen loss: 3.202909] \n",
            "[Epoch 973/2200]  [Disc loss: -0.628499] [Gen loss: 3.134904] \n",
            "[Epoch 974/2200]  [Disc loss: -0.627385] [Gen loss: 3.213775] \n",
            "[Epoch 975/2200]  [Disc loss: -0.632527] [Gen loss: 3.069472] \n",
            "[Epoch 976/2200]  [Disc loss: -0.626838] [Gen loss: 3.057532] \n",
            "[Epoch 977/2200]  [Disc loss: -0.626175] [Gen loss: 3.238314] \n",
            "[Epoch 978/2200]  [Disc loss: -0.626642] [Gen loss: 3.229098] \n",
            "[Epoch 979/2200]  [Disc loss: -0.627195] [Gen loss: 3.002086] \n",
            "[Epoch 980/2200]  [Disc loss: -0.629398] [Gen loss: 3.205899] \n",
            "[Epoch 981/2200]  [Disc loss: -0.629007] [Gen loss: 3.211689] \n",
            "[Epoch 982/2200]  [Disc loss: -0.621540] [Gen loss: 3.101542] \n",
            "[Epoch 983/2200]  [Disc loss: -0.627425] [Gen loss: 3.114512] \n",
            "[Epoch 984/2200]  [Disc loss: -0.622408] [Gen loss: 3.177550] \n",
            "[Epoch 985/2200]  [Disc loss: -0.626945] [Gen loss: 3.125092] \n",
            "[Epoch 986/2200]  [Disc loss: -0.623893] [Gen loss: 3.198217] \n",
            "[Epoch 987/2200]  [Disc loss: -0.631861] [Gen loss: 3.187400] \n",
            "[Epoch 988/2200]  [Disc loss: -0.624856] [Gen loss: 3.139622] \n",
            "[Epoch 989/2200]  [Disc loss: -0.626501] [Gen loss: 3.088297] \n",
            "[Epoch 990/2200]  [Disc loss: -0.621990] [Gen loss: 3.067154] \n",
            "[Epoch 991/2200]  [Disc loss: -0.623691] [Gen loss: 3.139510] \n",
            "[Epoch 992/2200]  [Disc loss: -0.623075] [Gen loss: 3.033893] \n",
            "[Epoch 993/2200]  [Disc loss: -0.622143] [Gen loss: 3.042730] \n",
            "[Epoch 994/2200]  [Disc loss: -0.625886] [Gen loss: 3.048489] \n",
            "[Epoch 995/2200]  [Disc loss: -0.623079] [Gen loss: 3.026957] \n",
            "[Epoch 996/2200]  [Disc loss: -0.623093] [Gen loss: 3.021321] \n",
            "[Epoch 997/2200]  [Disc loss: -0.624176] [Gen loss: 3.070620] \n",
            "[Epoch 998/2200]  [Disc loss: -0.627520] [Gen loss: 3.167593] \n",
            "[Epoch 999/2200]  [Disc loss: -0.622134] [Gen loss: 3.005666] \n",
            "[Epoch 1000/2200]  [Disc loss: -0.622678] [Gen loss: 3.092385] \n",
            "[Epoch 1001/2200]  [Disc loss: -0.620589] [Gen loss: 3.026636] \n",
            "[Epoch 1002/2200]  [Disc loss: -0.625566] [Gen loss: 3.023403] \n",
            "[Epoch 1003/2200]  [Disc loss: -0.619878] [Gen loss: 3.050327] \n",
            "[Epoch 1004/2200]  [Disc loss: -0.628226] [Gen loss: 3.066712] \n",
            "[Epoch 1005/2200]  [Disc loss: -0.625240] [Gen loss: 3.115399] \n",
            "[Epoch 1006/2200]  [Disc loss: -0.621487] [Gen loss: 3.173823] \n",
            "[Epoch 1007/2200]  [Disc loss: -0.621012] [Gen loss: 2.990418] \n",
            "[Epoch 1008/2200]  [Disc loss: -0.621364] [Gen loss: 3.035840] \n",
            "[Epoch 1009/2200]  [Disc loss: -0.622464] [Gen loss: 3.121182] \n",
            "[Epoch 1010/2200]  [Disc loss: -0.617257] [Gen loss: 3.014802] \n",
            "[Epoch 1011/2200]  [Disc loss: -0.621448] [Gen loss: 3.059814] \n",
            "[Epoch 1012/2200]  [Disc loss: -0.620851] [Gen loss: 3.152807] \n",
            "[Epoch 1013/2200]  [Disc loss: -0.622130] [Gen loss: 3.032848] \n",
            "[Epoch 1014/2200]  [Disc loss: -0.621186] [Gen loss: 3.070622] \n",
            "[Epoch 1015/2200]  [Disc loss: -0.625485] [Gen loss: 3.047955] \n",
            "[Epoch 1016/2200]  [Disc loss: -0.616432] [Gen loss: 2.995365] \n",
            "[Epoch 1017/2200]  [Disc loss: -0.620421] [Gen loss: 2.964510] \n",
            "[Epoch 1018/2200]  [Disc loss: -0.620785] [Gen loss: 3.073958] \n",
            "[Epoch 1019/2200]  [Disc loss: -0.622581] [Gen loss: 2.895802] \n",
            "[Epoch 1020/2200]  [Disc loss: -0.623169] [Gen loss: 3.093898] \n",
            "[Epoch 1021/2200]  [Disc loss: -0.617241] [Gen loss: 2.972954] \n",
            "[Epoch 1022/2200]  [Disc loss: -0.625309] [Gen loss: 3.029290] \n",
            "[Epoch 1023/2200]  [Disc loss: -0.624009] [Gen loss: 3.014894] \n",
            "[Epoch 1024/2200]  [Disc loss: -0.621219] [Gen loss: 2.936987] \n",
            "[Epoch 1025/2200]  [Disc loss: -0.620437] [Gen loss: 2.990702] \n",
            "[Epoch 1026/2200]  [Disc loss: -0.617431] [Gen loss: 3.013010] \n",
            "[Epoch 1027/2200]  [Disc loss: -0.615198] [Gen loss: 2.971218] \n",
            "[Epoch 1028/2200]  [Disc loss: -0.621524] [Gen loss: 2.921650] \n",
            "[Epoch 1029/2200]  [Disc loss: -0.620382] [Gen loss: 3.053848] \n",
            "[Epoch 1030/2200]  [Disc loss: -0.618284] [Gen loss: 2.891090] \n",
            "[Epoch 1031/2200]  [Disc loss: -0.619917] [Gen loss: 3.099976] \n",
            "[Epoch 1032/2200]  [Disc loss: -0.617977] [Gen loss: 3.079596] \n",
            "[Epoch 1033/2200]  [Disc loss: -0.619197] [Gen loss: 2.917972] \n",
            "[Epoch 1034/2200]  [Disc loss: -0.618074] [Gen loss: 3.242926] \n",
            "[Epoch 1035/2200]  [Disc loss: -0.618467] [Gen loss: 3.113626] \n",
            "[Epoch 1036/2200]  [Disc loss: -0.613288] [Gen loss: 2.981952] \n",
            "[Epoch 1037/2200]  [Disc loss: -0.619846] [Gen loss: 3.090348] \n",
            "[Epoch 1038/2200]  [Disc loss: -0.615984] [Gen loss: 2.973442] \n",
            "[Epoch 1039/2200]  [Disc loss: -0.613842] [Gen loss: 2.876612] \n",
            "[Epoch 1040/2200]  [Disc loss: -0.614152] [Gen loss: 3.103865] \n",
            "[Epoch 1041/2200]  [Disc loss: -0.616087] [Gen loss: 2.981557] \n",
            "[Epoch 1042/2200]  [Disc loss: -0.619638] [Gen loss: 2.930841] \n",
            "[Epoch 1043/2200]  [Disc loss: -0.615206] [Gen loss: 2.979637] \n",
            "[Epoch 1044/2200]  [Disc loss: -0.617341] [Gen loss: 2.974870] \n",
            "[Epoch 1045/2200]  [Disc loss: -0.614116] [Gen loss: 3.060865] \n",
            "[Epoch 1046/2200]  [Disc loss: -0.615934] [Gen loss: 2.947209] \n",
            "[Epoch 1047/2200]  [Disc loss: -0.613038] [Gen loss: 3.035539] \n",
            "[Epoch 1048/2200]  [Disc loss: -0.618831] [Gen loss: 2.912673] \n",
            "[Epoch 1049/2200]  [Disc loss: -0.620410] [Gen loss: 3.037008] \n",
            "[Epoch 1050/2200]  [Disc loss: -0.611875] [Gen loss: 2.980911] \n",
            "[Epoch 1051/2200]  [Disc loss: -0.618495] [Gen loss: 3.017039] \n",
            "[Epoch 1052/2200]  [Disc loss: -0.617184] [Gen loss: 3.069383] \n",
            "[Epoch 1053/2200]  [Disc loss: -0.615803] [Gen loss: 2.914386] \n",
            "[Epoch 1054/2200]  [Disc loss: -0.617179] [Gen loss: 3.083652] \n",
            "[Epoch 1055/2200]  [Disc loss: -0.613444] [Gen loss: 2.998355] \n",
            "[Epoch 1056/2200]  [Disc loss: -0.608875] [Gen loss: 2.956657] \n",
            "[Epoch 1057/2200]  [Disc loss: -0.610858] [Gen loss: 2.984947] \n",
            "[Epoch 1058/2200]  [Disc loss: -0.617114] [Gen loss: 3.045243] \n",
            "[Epoch 1059/2200]  [Disc loss: -0.615902] [Gen loss: 3.056838] \n",
            "[Epoch 1060/2200]  [Disc loss: -0.607960] [Gen loss: 3.200913] \n",
            "[Epoch 1061/2200]  [Disc loss: -0.615353] [Gen loss: 2.979168] \n",
            "[Epoch 1062/2200]  [Disc loss: -0.617563] [Gen loss: 2.981006] \n",
            "[Epoch 1063/2200]  [Disc loss: -0.611957] [Gen loss: 2.998805] \n",
            "[Epoch 1064/2200]  [Disc loss: -0.610448] [Gen loss: 3.116468] \n",
            "[Epoch 1065/2200]  [Disc loss: -0.618022] [Gen loss: 3.082771] \n",
            "[Epoch 1066/2200]  [Disc loss: -0.615852] [Gen loss: 2.945180] \n",
            "[Epoch 1067/2200]  [Disc loss: -0.609991] [Gen loss: 3.052370] \n",
            "[Epoch 1068/2200]  [Disc loss: -0.611705] [Gen loss: 2.996053] \n",
            "[Epoch 1069/2200]  [Disc loss: -0.611433] [Gen loss: 3.175380] \n",
            "[Epoch 1070/2200]  [Disc loss: -0.612768] [Gen loss: 2.919661] \n",
            "[Epoch 1071/2200]  [Disc loss: -0.608300] [Gen loss: 2.970187] \n",
            "[Epoch 1072/2200]  [Disc loss: -0.610359] [Gen loss: 3.157681] \n",
            "[Epoch 1073/2200]  [Disc loss: -0.611235] [Gen loss: 3.134624] \n",
            "[Epoch 1074/2200]  [Disc loss: -0.610498] [Gen loss: 3.100936] \n",
            "[Epoch 1075/2200]  [Disc loss: -0.610976] [Gen loss: 3.042741] \n",
            "[Epoch 1076/2200]  [Disc loss: -0.613094] [Gen loss: 3.113194] \n",
            "[Epoch 1077/2200]  [Disc loss: -0.610404] [Gen loss: 3.071494] \n",
            "[Epoch 1078/2200]  [Disc loss: -0.612452] [Gen loss: 3.038738] \n",
            "[Epoch 1079/2200]  [Disc loss: -0.603332] [Gen loss: 3.155719] \n",
            "[Epoch 1080/2200]  [Disc loss: -0.610169] [Gen loss: 3.089094] \n",
            "[Epoch 1081/2200]  [Disc loss: -0.601685] [Gen loss: 2.995527] \n",
            "[Epoch 1082/2200]  [Disc loss: -0.609232] [Gen loss: 2.941916] \n",
            "[Epoch 1083/2200]  [Disc loss: -0.613899] [Gen loss: 2.993437] \n",
            "[Epoch 1084/2200]  [Disc loss: -0.613195] [Gen loss: 3.033814] \n",
            "[Epoch 1085/2200]  [Disc loss: -0.606666] [Gen loss: 3.077523] \n",
            "[Epoch 1086/2200]  [Disc loss: -0.608056] [Gen loss: 3.103114] \n",
            "[Epoch 1087/2200]  [Disc loss: -0.608746] [Gen loss: 2.893357] \n",
            "[Epoch 1088/2200]  [Disc loss: -0.611764] [Gen loss: 3.045232] \n",
            "[Epoch 1089/2200]  [Disc loss: -0.613049] [Gen loss: 3.215706] \n",
            "[Epoch 1090/2200]  [Disc loss: -0.604956] [Gen loss: 2.998458] \n",
            "[Epoch 1091/2200]  [Disc loss: -0.609189] [Gen loss: 3.004542] \n",
            "[Epoch 1092/2200]  [Disc loss: -0.607720] [Gen loss: 3.188348] \n",
            "[Epoch 1093/2200]  [Disc loss: -0.606088] [Gen loss: 3.103525] \n",
            "[Epoch 1094/2200]  [Disc loss: -0.610262] [Gen loss: 3.134694] \n",
            "[Epoch 1095/2200]  [Disc loss: -0.604705] [Gen loss: 2.949215] \n",
            "[Epoch 1096/2200]  [Disc loss: -0.605011] [Gen loss: 3.103728] \n",
            "[Epoch 1097/2200]  [Disc loss: -0.610207] [Gen loss: 3.116681] \n",
            "[Epoch 1098/2200]  [Disc loss: -0.600755] [Gen loss: 3.215864] \n",
            "[Epoch 1099/2200]  [Disc loss: -0.608794] [Gen loss: 3.117002] \n",
            "[Epoch 1100/2200]  [Disc loss: -0.604097] [Gen loss: 3.174812] \n",
            "[Epoch 1101/2200]  [Disc loss: -0.604834] [Gen loss: 3.119963] \n",
            "[Epoch 1102/2200]  [Disc loss: -0.607310] [Gen loss: 3.059869] \n",
            "[Epoch 1103/2200]  [Disc loss: -0.604114] [Gen loss: 3.104939] \n",
            "[Epoch 1104/2200]  [Disc loss: -0.605259] [Gen loss: 3.274996] \n",
            "[Epoch 1105/2200]  [Disc loss: -0.608753] [Gen loss: 2.975076] \n",
            "[Epoch 1106/2200]  [Disc loss: -0.601148] [Gen loss: 3.060459] \n",
            "[Epoch 1107/2200]  [Disc loss: -0.604382] [Gen loss: 3.207433] \n",
            "[Epoch 1108/2200]  [Disc loss: -0.603137] [Gen loss: 3.022967] \n",
            "[Epoch 1109/2200]  [Disc loss: -0.604716] [Gen loss: 3.052866] \n",
            "[Epoch 1110/2200]  [Disc loss: -0.607295] [Gen loss: 3.015473] \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eTARoSEVlICZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.no_grad()\n",
        "trainer.G.eval()\n",
        "\n",
        "S = Sampler(generator=trainer.G)\n",
        "latent = S.sample(10) #10 samples\n",
        "latent = latent.detach().cpu().numpy().tolist()\n",
        "\n",
        "sampled_mols_save_path = os.path.join(trainer.output_model_folder, 'sampled')\n",
        "np.save(sampled_mols_save_path+f'_epoch{200}', latent)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w1VyOt8pH0Dn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x = np.load('/content/model/sampled_epoch200.npy')\n",
        "pd.DataFrame(x).head(10)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "avXnOi9WH9tg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}